---
title: "R Notebook"
---

# Linear regression

```{r}
source("include.R") 
```

jags to work on R 
```{r}
#Load the library
library(rjags)   # to interface R with JAGS
#if jags not installed gives error with link and instructions to install it for the first time
 
library(coda)        
library(plotrix)    # to  plot CIs

library(glmnet) # lasso
```





# Start dividing stations
```{r}
dati = df_weekly[,c(2,7)]
head(dati)
# df_stat=hash() ????
stations = unique(dati$IDStations)
print(stations)
length(stations)
```


Can stay here to see detailed on one station and plots or jump to for cycle storing betas for each station


# Analysis of one particular station
Create variables for regression analysis
```{r}
#remove column pm_10 (target)
#for now only using numerical covariates
st="1269"   #start with an example, then will cycle on all

y=df_weekly[which(df_weekly$IDStations==st),7]
y=as.numeric(y[[1]])
y=scale(y)[1:53]    #normalization on the station

covs=df_weekly[which(df_weekly$IDStations==st),3:39]
covs=covs[,-c(3,5,9,19,36)]    #not numerical
covs=covs[,-c(1,2,3,31)]   #lat, long, alti and land_use constant over same station
k=dim(covs)[2] # number of covariates

covs=as.data.frame(covs)
covariates=scale(covs)     #normalization on the station
#jitter to avoid nans in constant columns
for(i in 1:k){
  if(sum(is.na(covariates[,i]))>0)
    covs[,i]=jitter(covs[,i],0.01)
}
covs=scale(covs)


head(covs)
```

# check scaling
```{r}
par(mar=c(15,9,1,1))
boxplot(covs,las=2)

for (i in 1:size(covs)[2]){
	cat(crayon::red(colnames(covs)[i]))
	cat(" var =",var(covs[,i]),"\n")
}
```



# prepare data
```{r}
data = list(y=y,
            x=covs,
            n=dim(covs)[1],
            k=dim(covs)[2])
```

### jags stuff
Let's fix a list of initial value for the MCMC algorithm that JAGS will implement:
```{r}
inits = function() {list(beta0=0,
                         beta=rep(0,k),
                         sigma=100) }
```



# FIRST STEP
```{r echo=T, results='hide'}
modelRegress=jags.model("LinearReg.bug",data=data,inits=inits,n.adapt=1000,n.chains=1)
```
By default n.adapt=1000.
sigma is Uniform on the interval (0,100)

Prior not conjugate (?)


# SECOND STEP
```{r echo=T, results='hide'}
update(modelRegress,n.iter=5000) #this is the burn-in period
## this function DOES NOT record the sampled values of parameters
```
# THIRD STEP 
we tell JAGS to generate MCMC samples that we will use to represent the posterior distribution 
```{r}
variable.names=c("beta0", "beta", "sigma") #parameters - see the file .bug - that will have 
                                           # their values recorded
n.iter=50000 
thin=10  
## the final sample size, i.e. the number of iterations to build the ergodic average, will be 5K

```

## output regression
```{r echo=T, results='hide'}
outputRegress=coda.samples(model=modelRegress,variable.names=variable.names,n.iter=n.iter,thin=thin)
```

The OUTPUT is mcmc.list object - coda-formatted object; it needs to be converted into a matrix, in order to be "readable".



# OUTPUT ANALYSIS
```{r}
#### save(outputRegress,file='Jackman_regr_output.Rdata')

##### upload the output if we have previuosly stored it and we don't want to run the MCMC again
#### load('Jackman_regr_output.Rdata')

data.out=as.matrix(outputRegress) # trasform the mcmc.list into a matrix,
data.out=data.frame(data.out)     # or, better, into a dataframe (easiest to handle in R)
attach(data.out)
n.chain=dim(data.out)[1] 
n.chain # this is the final sample size
```

# Summary statistics for the posterior 
Use either packege CODA, or standard tools in R:
```{r}
size(data.out)

summary(data.out)
head(data.out)
```

## Autocorrelation plots
```{r}
par(mfrow=c(1,3))
par(mar=c(2,2,3,1)) # bottom, left, top, right margins to not cut plot titles

acf(data.out[,'beta0'],lwd=3,col="red3",main="autocorrelation of beta0")
acf(data.out[,'beta.1.'],lwd=3,col="red3",main="autocorrelation of beta1")
acf(data.out[,'sigma'],lwd=3,col="red3",main="autocorrelation of sigma_res")
```
## nice graphs posterior
```{r}
#sub-chain containing the beta sample
beta.post <- data.out[,1:(k+1)]
#posterior mean of the beta parameters
beta.bayes  <- apply(beta.post,2,"mean")
beta.bayes <- c(beta.bayes[k+1],beta.bayes[1:k])  #just to have beta0 in first position 
beta.bayes
#sub-chain whit the sigma_res samle
sig.post= data.out[,'sigma']
```

## Comparison frequentist- bayesian
```{r}
#### MLE estimate
mod= lm(y ~ .,data = data.frame(covs))
#summary(mod)

mod$coefficients  # Comparison 
beta.bayes
```
# Posterior of $\beta_0$
```{r}
## Representation of the posterior chain of  beta0
chain <- beta.post[,k+1]
#Divide the plot device in three sub-graph regions
#two square on the upper and a rectangle on the bottom

par(mar=c(2,2,3,2)) # bottom, left, top, right margins to not cut plot titles
layout(matrix(c(1,2,3,3),2,2,byrow=T))
#trace-plot of the posterior chain
plot(chain,type="l",main="Trace plot of beta0")
# autocorrelation plot
acf(chain,lwd=3,col="red3",main="autocorrelation of beta0")
#Histogram
hist(chain,nclass="fd",freq=F,main="Posterior of beta0",col="gray") 
## Overlap the kernel-density 
lines(density(chain),col="blue",lwd=2)
## Posterior credible interval of beta0
quantile(chain,prob=c(0.025,0.5,0.975))

## Display the posterior credible interval on the graph
abline(v=quantile(chain,prob=c(0.025)),col="red",lty=2,lwd=2)
abline(v=quantile(chain,prob=c(0.5)),col="red",lty=1,lwd=2)
abline(v=quantile(chain,prob=c(0.975)),col="red",lty=2,lwd=2)
## Add the legend to the plot
legend("topright",legend=c("posterior median", "95% Credible bounds","kernel density smoother"),lwd=c(2,2,2), col=c("red","red","blue"),lty=c(1,2,1))

```

# Posterior of $\beta_1$
```{r}
chain <- beta.post[,1]
#Divide the plot device 
par(mar=c(2,2,3,2)) # bottom, left, top, right margins to not cut plot titles
layout(matrix(c(1,2,3,3),2,2,byrow=T))
#trace-plot of the posterior chain
plot(chain,type="l",main="Trace plot of beta1")
# autocorrelation plot
acf(chain,lwd=3,col="red3",main="autocorrelation of beta1")
#Histogram
hist(chain,nclass="fd",freq=F,main="Posterior of beta1",col="gray") 
## Overlap the kernel-density 
lines(density(chain),col="blue",lwd=2)
#### Posterior credible interval of  beta1
quantile(chain,prob=c(0.025,0.5,0.975))

## Display the posterior credible interval on the graph
abline(v=quantile(chain,prob=c(0.025)),col="red",lty=2,lwd=2)
abline(v=quantile(chain,prob=c(0.5)),col="red",lty=1,lwd=2)
abline(v=quantile(chain,prob=c(0.975)),col="red",lty=2,lwd=2)
## Add the legend to the plot
legend("topright",legend=c("posterior median", "95% Credible bounds","kernel density smoother"),lwd=c(2,2,2), col=c("red","red","blue"),lty=c(1,2,1))
```

## Posterior of $\beta_i$
```{r}
for (i in 1:k+1){
	chain <- beta.post[,i]
	#Divide the plot device 
	par(mar=c(2,2,3,2)) # bottom, left, top, right margins to not cut plot titles
	layout(matrix(c(1,2,3,3),2,2,byrow=T))
	#trace-plot of the posterior chain
	plot(chain,type="l",main=paste("Trace plot of beta",i))
	# autocorrelation plot
	acf(chain,lwd=3,col="red3",main=paste("autocorrelation of beta",i))
	#Histogram
	hist(chain,nclass="fd",freq=F,main=paste("Posterior of beta",i),col="gray") 
	## Overlap the kernel-density 
	lines(density(chain),col="blue",lwd=2)
	#### Posterior credible interval of  beta1
	# quantile(chain,prob=c(0.025,0.5,0.975))
	
	## Display the posterior credible interval on the graph
	abline(v=quantile(chain,prob=c(0.025)),col="red",lty=2,lwd=2)
	abline(v=quantile(chain,prob=c(0.5)),col="red",lty=1,lwd=2)
	abline(v=quantile(chain,prob=c(0.975)),col="red",lty=2,lwd=2)
	## Add the legend to the plot
	legend("topright",legend=c("posterior median", "95% Credible bounds","kernel density smoother"),lwd=c(2,2,2), col=c("red","red","blue"),lty=c(1,2,1))
}
```
# Posterior of $\sigma$: 
```{r}
##Representation of the posterior chain of sigma_res
chain <-sig.post
#Divide the plot device 
par(mar=c(2,2,3,2)) # bottom, left, top, right margins to not cut plot titles
layout(matrix(c(1,2,3,3),2,2,byrow=T))
#trace-plot of the posterior chain
plot(chain,type="l",main="Trace plot of sigma_res")
# autocorrelation plot
acf(chain,lwd=3,col="red3",main="autocorrelation of sigma_res")
#Histogram
hist(chain,nclass="fd",freq=F,main="Posterior of sigma_res",col="gray") 
## Overlap the kernel-density 
lines(density(chain),col="blue",lwd=2)
## Posterior Credible bound of sigma_res
quantile(chain,prob=c(0.05,0.5,0.95))

## Display the posterior credible interval on the graph
abline(v=quantile(chain,prob=c(0.025)),col="red",lty=2,lwd=2)
abline(v=quantile(chain,prob=c(0.5)),col="red",lty=1,lwd=2)
abline(v=quantile(chain,prob=c(0.975)),col="red",lty=2,lwd=2)
## Add the legend to the plot
legend("topright",legend=c("posterior median", "95% Credible bounds","kernel density smoother"),lwd=c(2,2,2), col=c("red","red","blue"),lty=c(1,2,1))
```


```{r}
detach(data.out)
```









# saving all betas all stations
```{r}
bayesian_coefficients <- data.frame()
  
frequentist_coefficients <- data.frame()

vars_names = colnames(covs)

# stuff for lasso
formula <- " y ~ ."
lambda.grid <- 10^seq(5,-3,length=100)
soglia = 0.001
best_lasso_vect = c()

best_variables_lasso <- data.frame(variables = c("(Intercept)",vars_names), checked = rep(0,(length(vars_names)+1)))


```


```{r}
# It's the same code as before, but in a 'for' cycle

for (st in stations){
	cat(crayon::red("Station",st,"\n"))
 #covariates and target
  y=df_weekly[which(df_weekly$IDStations==st),"AQ_pm10"]
  y=as.numeric(y[[1]])
  y=scale(y)[1:53]    #normalization on each single station ( 53 weeks)
  cat(crayon::blue(all(y>0)))

  covs=df_weekly[which(df_weekly$IDStations==st),vars_names]
  k=dim(covs)[2]
  
  covs=as.data.frame(covs)
  covariates=scale(covs)     #normalization on each single station
  #jitter to avoid nans in constant columns
  for(i in 1:k){
    if(sum(is.na(covariates[,i]))>0)
      covs[,i]=jitter(covs[,i],0.01)
  }
  covs=scale(covs)


 #build model
  data = list(y=y,
            x=covs,
            n=dim(covs)[1],
            k=k)
  inits = function() {list(beta0=0,
                         beta=rep(0,k),
                         sigma=100) }

#run model  
  modelRegress=jags.model("LinearReg.bug",data=data,inits=inits,n.adapt=1000,n.chains=1)
  update(modelRegress,n.iter=5000) #this is the burn-in period
  
 #save betas and sigma 
  variable.names=c("beta0", "sigma","beta" ) #parameters 
  n.iter=50000 
  thin=10  
  outputRegress=coda.samples(model=modelRegress,variable.names=variable.names,n.iter=n.iter,thin=thin)
  
  data.out=as.matrix(outputRegress) # trasform the mcmc.list into a matrix,
  data.out=data.frame(data.out)     # or, better, into a dataframe (easiest to handle in R)
  attach(data.out)
  n.chain=dim(data.out)[1] 

 #the betas
  beta.post <- data.out[,0:k+1]   #posterior mean of the beta parameters
  beta.bayes  <- apply(beta.post,2,"mean")
  beta.bayes <- c(beta.bayes[k+1],beta.bayes[1:k])  #just to put beta0 in first position
  bayesian_coefficients <- rbind(bayesian_coefficients,c(st,beta.bayes))
  
 #linear (frequentist)
  linear_model_classic= lm(y ~ .,data = data.frame(covs))
  frequentist_coefficients <- rbind(frequentist_coefficients,c(st,linear_model_classic$coefficients))
  
  # LASSO
  y = df_weekly$AQ_pm10[which(df_weekly$IDStations==st)]
  x <- model.matrix(as.formula(formula),data.frame(covs))[,-1]
  fit.lasso <- glmnet(x,y, lambda = lambda.grid) 
  cv.lasso <- cv.glmnet(x,y,lambda=lambda.grid)

  bestlam.lasso <- cv.lasso$lambda.min
  best_lasso_vect = c(best_lasso_vect,bestlam.lasso)

  coef.lasso <- predict(fit.lasso, s=bestlam.lasso, type = 'coefficients')
  coef.lasso =  data.frame(coef.lasso[,1])
  coef.lasso$variables = rownames(coef.lasso)
  colnames(coef.lasso)[1] = "coeff"
  final_coeff = coef.lasso[which(abs(coef.lasso$coeff)>soglia),"variables"]
  
  best_variables_lasso[which(best_variables_lasso$variables%in%final_coeff),"checked"] = best_variables_lasso[which(best_variables_lasso$variables%in%final_coeff),"checked"] +1
  
    
  detach(data.out)
  
  
}
#to give columns names
colnames(bayesian_coefficients)<-c('station',names(beta.bayes))
colnames(frequentist_coefficients)<-c('station',names(beta.bayes))

```



#present results
```{r}
head(bayesian_coefficients)
head(frequentist_coefficients)
best_lasso_vect
best_variables_lasso
```
# Lasso regression classic model
```{r}
formula <- " y ~ ."
linear_model_classic <- lm(formula,data = data.frame(covs))
summary(linear_model_classic)

#predictors matrix and response vector
x <- model.matrix(as.formula(formula),data.frame(covs))[,-1]
y = df_weekly$AQ_pm10[which(df_weekly$IDStations==st)]

lambda.grid <- 10^seq(5,-3,length=100)
fit.lasso <- glmnet(x,y, lambda = lambda.grid) 

plot(fit.lasso,xvar='lambda',label=TRUE, col = rainbow(dim(x)[2]))
legend('topright', dimnames(x)[[2]], col =  rainbow(dim(x)[2]), lty=1, cex=1)

#cross validation for lambda
cv.lasso <- cv.glmnet(x,y,lambda=lambda.grid)

bestlam.lasso <- cv.lasso$lambda.min
bestlam.lasso

plot(cv.lasso)
abline(v=log(bestlam.lasso), lty=1)


coef.lasso <- predict(fit.lasso, s=bestlam.lasso, type = 'coefficients')
coef.lasso =  data.frame(coef.lasso[,1])
coef.lasso$variables = rownames(coef.lasso)
colnames(coef.lasso)[1] = "coeff"
final_coeff = coef.lasso[which(abs(coef.lasso$coeff)>0.001),"variables"]


```






