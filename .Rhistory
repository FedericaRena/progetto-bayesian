#prior.Sigma.df=NULL,
#prior.Sigma.scale=NULL,
AR=1, #1 or 2
rho.S=NULL,
rho.T=NULL,
MALA=TRUE,
verbose=TRUE)
mod<-ST.CARadaptive(formula= formula,
family = "gaussian",
data=covariates,
W= dist_matrix,
burnin = 10,
n.sample = 1000,
thin=1,
prior.mean.beta=NULL,
prior.var.beta=NULL,
prior.nu2=NULL,
MALA=TRUE,
verbose=TRUE)
modelfit(mod)
mod<-ST.CARar(formula= formula,
family = "gaussian",
data=covariates,
W= dist_matrix,
burnin = 10,
n.sample = 1000,
thin=1,
n.chains=1,
n.cores=1,
prior.mean.beta=NULL,
prior.var.beta=NULL,
prior.nu2=NULL,
#prior.Sigma.df=NULL,
#prior.Sigma.scale=NULL,
AR=1, #1 or 2
rho.S=NULL,
rho.T=NULL,
MALA=TRUE,
verbose=TRUE)
accept(mod)
plot(mod$accept, type = "l", ylab = "Acceptance Probability", xlab = "Iteration")
mod$accept
modelfit(mod)
modÂ£$modelfit
mod$modelfit
summary(mod$samples)
summary_results = summary(mod)
# Identify variables with credible intervals excluding zero
selected_variables <- summary_results[summary_results$`2.5%` > 0 | summary_results$`97.5%` < 0, ]
summary_results$`2.5%`
summary_results$"2.5%"
summary_results
summary_results = summary(mod$beta)
summary_results
summary_results = mod$summary.results
summary_results
mod$mcmc.info
summary_results = mod$summary.results
# Identify variables with credible intervals excluding zero
selected_variables <- summary_results[summary_results$"2.5%" > 0 | summary_results$`97.5%` < 0, ]
summary_results$"2.5%"
names(summary_results)
summary.results
summary_results
summary_results = data.frame(summary_results)
summary_results
summary_results = data.frame(summary_results)
# Identify variables with credible intervals excluding zero
selected_variables <- summary_results[summary_results$X2.5. > 0 | summary_results$X97.5. < 0, ]
# Print the selected variables
print("Selected Variables:")
print(selected_variables)
source("include.R")
library(CARBayesST)
ordered_by_time <- df_wsc[order(df_wsc$Time), ]
head(ordered_by_time)
colnames(ordered_by_time)
covariates_to_exclude = c("X","IDStations",
"Latitude","Longitude","Time","day","week"
)
covariates=ordered_by_time[,-which(colnames(ordered_by_time)%in% covariates_to_exclude)]
head(covariates)
library(sp)
library(proxy)
spatial_coord = df_wsc[df_wsc$Time=="2018-01-01",c("IDStations","Latitude","Longitude")]
head(spatial_coord)
coordinates(spatial_coord) <- c("Longitude", "Latitude")
# Calculate the pairwise Euclidean distance matrix
dist_matrix <- as.matrix(proxy::dist(coordinates(spatial_coord), method = "Euclidean"))
formula = as.formula("AQ_pm10~.")
# Load required libraries
library(MASS)  # For mvrnorm function (multivariate normal distribution)
library(spdep) # For spatial dependency functions
library(INLA)   # For ST.CARanova, if applicable
install.packages("INLA")
# Load required libraries
library(MASS)  # For mvrnorm function (multivariate normal distribution)
library(spdep) # For spatial dependency functions
library(stats)  # For kronecker function
# Setup the regular lattice
x.easting <- 1:10
x.northing <- 1:10
Grid <- expand.grid(x.easting, x.northing)
K <- nrow(Grid)
N <- 10
N.all <- N * K
# Setup spatial (W) and temporal (D) neighbourhood matrices
distance <- as.matrix(dist(Grid))
W <- array(0, c(K, K))
W[distance == 1] <- 1
D <- array(0, c(N, N))
for (i in 1:N) {
for (j in 1:N) {
if (abs((i - j)) == 1) D[i, j] <- 1
}
}
# Simulate the elements in the linear predictor and the data
gamma <- rnorm(n = N.all, mean = 0, sd = 0.001)
x <- rnorm(n = N.all, mean = 0, sd = 1)
beta <- 0.1
Q.W <- 0.99 * (diag(apply(W, 2, sum)) - W) + 0.01 * diag(rep(1, K))
Q.W.inv <- solve(Q.W)
phi <- mvrnorm(n = 1, mu = rep(0, K), Sigma = (0.01 * Q.W.inv))
Q.D <- 0.99 * (diag(apply(D, 2, sum)) - D) + 0.01 * diag(rep(1, N))
Q.D.inv <- solve(Q.D)
phi.long <- rep(phi, N)
delta.long <- kronecker(delta, rep(1, K))
delta<-mvrnorm(n=1,mu=rep(0,N),Sigma=(0.01*Q.D.inv))
phi.long <- rep(phi, N)
delta.long <- kronecker(delta, rep(1, K))
LP <- 4 + x * beta + phi.long + delta.long + gamma
mean <- exp(LP)
Y <- rpois(n = N.all, lambda = mean)
# Run the model (commented out for now)
model <- ST.CARanova(formula = Y ~ x, family = "poisson", interaction = TRUE, W = W, burnin = 10, n.sample = 50)
summary(model)
model$summary.results
print(mod)
summary_results = mod$summary.results
summary_results = data.frame(summary_results)
# Identify variables with credible intervals excluding zero
selected_variables <- summary_results[summary_results$X2.5. > 0 | summary_results$X97.5. < 0, ]
# Print the selected variables
print("Selected Variables:")
print(selected_variables)
coef(mod)
mod$accept
mod$modelfit
resid = residuals(mod)
plot(resid)
# Run the model (commented out for now)
mod <- ST.CARanova(formula = Y ~ x, family = "poisson", interaction = TRUE, W = W, burnin = 10, n.sample = 50)
print(mod)
summary_results = mod$summary.results
summary_results = data.frame(summary_results)
# Identify variables with credible intervals excluding zero
selected_variables <- summary_results[summary_results$X2.5. > 0 | summary_results$X97.5. < 0, ]
# Print the selected variables
print("Selected Variables:")
print(selected_variables)
coef(mod)
mod$accept
mod$modelfit
resid = residuals(mod)
plot(resid)
beta.mcmcsummary<-summary(mod$samples$beta)
round(RR.summary, 3)
ST.CARclustrends(formula= formula,
family = "gaussian",
data=covariates,
W= dist_matrix,
t
burnin = 10,
n.sample = 1000,
mod = ST.CARclustrends(formula= formula,
family = "gaussian",
data=covariates,
W= dist_matrix,
burnin = 10,
n.sample = 1000,
thin=1,
trends=NULL,
changepoint=NULL,
knots=NULL,
prior.mean.beta=NULL,
prior.var.beta=NULL,
prior.mean.gamma=NULL,
prior.var.gamma=NULL,
prior.lambda=NULL,
prior.tau2=NULL,
Nchains=4,
verbose=TRUE)
mod = ST.CARclustrends(formula= formula,
family = "poisson",
data=covariates,
W= dist_matrix,
burnin = 10,
n.sample = 1000,
thin=1,
trends=NULL,
changepoint=NULL,
knots=NULL,
prior.mean.beta=NULL,
prior.var.beta=NULL,
prior.mean.gamma=NULL,
prior.var.gamma=NULL,
prior.lambda=NULL,
prior.tau2=NULL,
Nchains=4,
verbose=TRUE)
mod = ST.CARclustrends(formula= formula,
family = "poisson",
data=covariates,
W= dist_matrix,
burnin = 10,
n.sample = 1000,
thin=1,
trends=trends=c("Constant","LI"),
mod = ST.CARclustrends(formula= formula,
family = "poisson",
data=covariates,
W= dist_matrix,
burnin = 10,
n.sample = 1000,
thin=1,
trends=c("Constant","LI"),
changepoint=NULL,
knots=NULL,
prior.mean.beta=NULL,
prior.var.beta=NULL,
prior.mean.gamma=NULL,
prior.var.gamma=NULL,
prior.lambda=NULL,
prior.tau2=NULL,
Nchains=4,
verbose=TRUE)
mod = ST.CARclustrends(formula= formula,
family = "binomial",
data=covariates,
W= dist_matrix,
burnin = 10,
n.sample = 1000,
thin=1,
trends=c("Constant","LI"),
changepoint=NULL,
knots=NULL,
prior.mean.beta=NULL,
prior.var.beta=NULL,
prior.mean.gamma=NULL,
prior.var.gamma=NULL,
prior.lambda=NULL,
prior.tau2=NULL,
Nchains=4,
verbose=TRUE)
mod<-ST.CARlinear(formula= formula,
family = "gaussian",
data=covariates,
W= dist_matrix,
burnin = 10,
n.sample = 1000,
thin=1,
n.chains=1,
n.cores=1,
prior.mean.beta=NULL,
prior.var.beta=NULL,
prior.mean.alpha=NULL,
prior.var.alpha=NULL,
prior.nu2=NULL,
prior.tau2=NULL,
rho.slo=NULL,
rho.int=NULL,
MALA=TRUE,
verbose=TRUE)
# CLUSTERends:
only binomial or poisson: response must be integer valued
# create a distance matrix optimized using altitude, does this make any sense?
altitude <-  df_wsc[df_wsc$Time=="2018-01-01","Altitude"]
altitude
library(sp)
library(proxy)
spatial_coord = df_wsc[df_wsc$Time=="2018-01-01",c("IDStations","Latitude","Longitude")]
head(spatial_coord)
coordinates(spatial_coord) <- c("Longitude", "Latitude")
# Calculate the pairwise Euclidean distance matrix
dist_matrix <- as.matrix(proxy::dist(coordinates(spatial_coord), method = "Euclidean"))
# create a distance matrix optimized using altitude, does this make any sense?
altitude <-  df_wsc[df_wsc$Time=="2018-01-01","Altitude"]
dist_matrix <- W.estimate(dist_matrix,spdata = altitude ,add=FALSE,remove=TRUE,remove_first=FALSE)
W.estimate(dist_matrix,spdata = altitude
,add=FALSE,
remove=TRUE,remove_first=FALSE)
# Calculate the pairwise Euclidean distance matrix
dist_matrix <- as.matrix(proxy::dist(coordinates(spatial_coord), method = "Euclidean"))
dist_matrix
image(dist_matrix)
# create a distance matrix optimized using altitude, does this make any sense?
altitude <-  df_wsc[df_wsc$Time=="2018-01-01","Altitude"]
dist_matrix <- W.estimate(dist_matrix,spdata = altitude
,add=FALSE,
remove=TRUE,remove_first=FALSE)
dist_matrix <- W.estimate(dist_matrix,spdata = NULL
,add=FALSE,
remove=TRUE,remove_first=FALSE)
beta.mcmcsummary<-summary(mod$samples$beta)
round(beta.mcmcsummary, 3)
source("include.R")
source("plot functions/plot_data_and_libraries.R")
source("include.R")
source("plot functions/plot_data_and_libraries.R")
img_lombardy <- readPNG("italia/lombardia_cut.png")
img_lombardy <- readPNG("/italia/lombardia_cut.png")
img_lombardy <- readPNG("./italia/lombardia_cut.png")
img_lombardy <- readPNG("./italia/lombardia.png")
img_lombardy <- readPNG("italia/lombardia.png")
img_lombardy <- readPNG("../italia/lombardia.png")
setwd("C:/Users/modin/Desktop/Ettore/UNIVERSITA/progetto-bayesian")
img_lombardy <- readPNG("../italia/lombardia.png")
img_lombardy <- readPNG("./src/plot functions/italia/lombardia.png")
# carica file shp
details <- 3
details_altre_regioni <- 2
confini  <- c("Emilia-Romagna","Piemonte","Lombardia","Trentino-Alto Adige","Veneto")
regioni_italiane <- st_read(paste0("plot functions/italia/gadm40_ITA_",details,".shp"))
# carica file shp
details <- 3
details_altre_regioni <- 2
confini  <- c("Emilia-Romagna","Piemonte","Lombardia","Trentino-Alto Adige","Veneto")
regioni_italiane <- st_read(paste0("./src/plot functions/italia/gadm40_ITA_",details,".shp"))
regioni_italiane_2 <- st_read(paste0("./src/plot functions/italia/gadm40_ITA_",details_altre_regioni,".shp"))
lombardia <- regioni_italiane[regioni_italiane$NAME_1 == "Lombardia",]
altre_regioni <- regioni_italiane_2[regioni_italiane_2$NAME_1 %in% confini,]
shp_map <- st_read(paste0("./src/plot functions/italia/gadm40_ITA_",1,".shp"))
source("plot functions/plot_utilities.R")
source("plot functions/plot_functions.R")
stationPlotgg <- stationPlot()
print(stationPlotgg)
DefaultPlot()
library(drpm)
library(salso)
library(grDevices) # for image saving
# preparation
source("include.R") # for having df_agri
source("plot functions/plotter.R")
source("plot functions/plotter.R")
library(drpm)
library(salso)
library(grDevices) # for image saving
# preparation
source("include.R") # for having df_agri
source("plot functions/plotter.R")
library(drpm)
library(salso)
library(grDevices) # for image saving
# preparation
source("include.R") # for having df_agri
source("plot functions/plotter.R")
library(drpm)
library(salso)
library(grDevices) # for image saving
# preparation
source("include.R") # for having df_agri
source("plot functions/plotter.R")
sites = data.frame(
longitude = unique(df_weekly$Longitude),
latitude = unique(df_weekly$Latitude))
source("include_clusters_functions.R")
stations = unique(df_wsc$IDStations)
y=data.frame()
for(st in stations){
y_we_pm10=cbind(as.data.frame(st),t(df_wsc[which(df_wsc$IDStations==st),"AQ_pm10"]))
y=rbind(y,y_we_pm10)
}
rownames(y) = NULL
colnames(y)<- c("id",paste0("w", 1:53))
df_wsc
y
cols = colora(105,56,show=F)
chosen_variable_name = "AQ_pm10"
trendYearStation_week <- function(file_name){
data_from_to = df_wsc
len_time = 54
chosen_variable = (data_from_to[,chosen_variable_name])
# Crea il grafico ggplot
station_trend <- ggplot(data_from_to,aes(x = week,
y = AQ_pm10,
group=IDStations,
color = as.factor(IDStations))) +
geom_line(show.legend = FALSE) +
labs(x = "Stations", y = chosen_variable_name, title = "Year: 2018 all stations") +
ylim(range(na.omit(chosen_variable))) +
scale_color_manual(values = cols) +
theme_bw()+
theme(panel.grid = element_blank()) +
guides(color = guide_legend())+
labs(x="week")
len_time = (len_time%/%5)
return(trend_animator(file_name,station_trend, data_from_to$week,len_time))
}
trendYearStation_week("None")
std_sites = data.frame(
longitude = unique(df_wsc$Longitude),
latitude = unique(df_wsc$Latitude))
std_sites
plot(sites)
plot(std_sites,col="blue")
time_span = 1:16 # low time span for quick testing, real one will be 1:53
### authors suggested to/did scale the spatial locations and also centered the observations
y_fit = y[,1+time_span]
tps = ncol(y_fit)
y_fit
### and for the scaling of spatial locations we already built
std_sites
# modelPriors = c(m0, s20, Asig, Atau, Alam, a, b, be)
# m0 - mean of phi0, phi0 is the mean of theta, theta is the mean of Y
# s20 - variance of theta, ecc
# Asig - maximum value for sigma
#	[0,Asig] is the support of the uniform in which sigma (std dev of Y) varies
# Atau - maximum value for tau
#	[0,Asig] is the support of the uniform in which mu (mean of Y) varies
# Alam - maximum value for lambda
# lambda is a unuform on [0,Alam]
# lambda is the var of tau, tau is the var of sigma, sigma is the var of Y
# a - shape 1 for alpha
# b - shape 2 for alpha
# b - scale for eta1
modelPriors <- c(0,100, 10, 5, 5, 2, 2, 1)
# m, k0, nu0, L0
# cParms <- c(0,1,5,1)
cParms <- c(0,1,5,1) # as authors
# SIG, TAU, LAM, ETA1, PHI1
# mh <- c(1,1, 1, 0.1, 0.1)
mh <- c(0.1,0.1, 0.7, 0.1, 0.1) # i changed this for sigma2 trace plots
sp <- 4
# niter=50000; nburn=10000; nthin=40 # real one
niter=30000; nburn=15000; nthin=15
nout <- (niter-nburn)/nthin
cat(nout,"valid iterations")
set.seed(1)
models.out <- list()
hh <- 1
# h <- "111"; s <- "0";
model <- "AR1"
# for(s in c("0","1")){ # we want to use space
# authors did also space comparison as it was the paper topic also
models_vector = c("E1P1A1","E1P1A0","E1P0A1","E1P0A0","E0P1A1","E0P1A0","E0P0A1","E0P0A0")
for(h in models_vector){
# readline(prompt="Press [enter] to continue fitting") # to let the pc breath
cat(paste0("fitting model ",hh," (",models_vector[hh],")...\n"))
m.n <- as.numeric(strsplit(h, "")[[1]][c(2,4,6)])
eta1_bool <- m.n[1]!=0
phi1_bool <- m.n[2]!=0
alphat_bool <- m.n[3]!=0
# we select true if the number in h was 1, ie "!=0"
# we always want to use space
# if(s=="0"){
# sc <- NULL
# } else {
sc <- std_sites
# }
tempo_inizio <- Sys.time()
set.seed(1*hh)
sink(nullfile()) # suppress output
out <- drpm_fit(draws=niter, burn=nburn, thin=nthin,
y=y_fit, M=1, s_coords=sc,
# global_alpha=FALSE, # forse intendevano questi due comandi dopo:
unit_specific_alpha=FALSE,
# maybe our tests should be concentrated here
# ie testing with this TRUE or FALSE
time_specific_alpha=alphat_bool, # meaning a bit ambiguos
# Ok after experimenting it means that:
# - if true we let alpha be a param that changes over time.
#	At time 1 we have a chain with some behaviour,
#	at time 2 another chain with another behaviour, ecc
# - if false we instead fix alpha, ie all chains of all times
#	will be the same, as they refer to the same parameter estimation
# The authors in their tests set it to false, ie the fixed alpha
modelPriors=modelPriors,
# ... as testing on this alpha_0 does not seem really relevant
# i mean alpha should always be let able to change
# if we set alpha_0 = TRUE we would never update alpha, but that's not our
# interest (it was in the interest of the paper marketing)
alpha_0 = FALSE,
eta1_0 = eta1_bool,
phi1_0 = phi1_bool,
SpatialCohesion=sp, cParms=cParms,mh=mh,
verbose=FALSE)
sink() # end suppress output
tempo_fine <- Sys.time()
cat(crayon::red("##############################\n"))
cat(crayon::cyan("Model is",paste0(h),"ie:"),
"\neta1_bool =",eta1_bool,"-",
"phi1_bool =",phi1_bool,"-",
"alphat_bool =",alphat_bool,"-",
"\n")
# cat("seed is", 1*hh, "\n")
cat("\n")
differenza_tempo <- tempo_fine - tempo_inizio
cat("Fit took:\n")
print(round(differenza_tempo,digits = 4))
# print(date())
cat(h,"lpml = ", out$lpml, "\n")
cat(h,"waic = ", out$waic, "\n")
cat(crayon::red("##############################\n"))
models.out[[hh]] <- out
names(models.out)[hh] <- paste0("out_",h,"_",model)
# rho <- list()
# for(k in 1:tps){
# 	# rho[[k]] <- salso(t(out$Si[k,,]), loss="VI")
# 	rho[[k]] <- salso(t(out$Si[k,,]), loss="binder")
# }
hh <- hh + 1
}
