layout(matrix(c(1,2,3,3),2,2,byrow=T))
#trace-plot of the posterior chain
plot(chain,type="l",main=paste("Trace plot of beta",i))
# autocorrelation plot
acf(chain,lwd=3,col="red3",main=paste("autocorrelation of beta",i))
#Histogram
hist(chain,nclass="fd",freq=F,main=paste("Posterior of beta",i),col="gray")
## Overlap the kernel-density
lines(density(chain),col="blue",lwd=2)
#### Posterior credible interval of  beta1
# quantile(chain,prob=c(0.025,0.5,0.975))
## Display the posterior credible interval on the graph
abline(v=quantile(chain,prob=c(0.025)),col="red",lty=2,lwd=2)
abline(v=quantile(chain,prob=c(0.5)),col="red",lty=1,lwd=2)
abline(v=quantile(chain,prob=c(0.975)),col="red",lty=2,lwd=2)
## Add the legend to the plot
legend("topright",legend=c("posterior median", "95% Credible bounds","kernel density smoother"),lwd=c(2,2,2), col=c("red","red","blue"),lty=c(1,2,1))
}
##Representation of the posterior chain of sigma_res
chain <-sig.post
#Divide the plot device
par(mar=c(2,2,3,2)) # bottom, left, top, right margins to not cut plot titles
layout(matrix(c(1,2,3,3),2,2,byrow=T))
#trace-plot of the posterior chain
plot(chain,type="l",main="Trace plot of sigma_res")
# autocorrelation plot
acf(chain,lwd=3,col="red3",main="autocorrelation of sigma_res")
#Histogram
hist(chain,nclass="fd",freq=F,main="Posterior of sigma_res",col="gray")
## Overlap the kernel-density
lines(density(chain),col="blue",lwd=2)
## Posterior Credible bound of sigma_res
quantile(chain,prob=c(0.05,0.5,0.95))
## Display the posterior credible interval on the graph
abline(v=quantile(chain,prob=c(0.025)),col="red",lty=2,lwd=2)
abline(v=quantile(chain,prob=c(0.5)),col="red",lty=1,lwd=2)
abline(v=quantile(chain,prob=c(0.975)),col="red",lty=2,lwd=2)
## Add the legend to the plot
legend("topright",legend=c("posterior median", "95% Credible bounds","kernel density smoother"),lwd=c(2,2,2), col=c("red","red","blue"),lty=c(1,2,1))
detach(data.out)
bayesian_coefficients <- data.frame()
frequentist_coefficients <- data.frame()
vars_names = colnames(covs)
# stuff for lasso
formula <- " y ~ ."
lambda.grid <- 10^seq(5,-3,length=100)
soglia = 0.001
best_lasso_vect = c()
best_variables_lasso <- data.frame(variables = c("(Intercept)",vars_names), checked = rep(0,(length(vars_names)+1)))
# It's the same code as before, but in a 'for' cycle
for (st in stations){
cat(crayon::red("Station",st,"\n"))
#covariates and target
y=df_weekly[which(df_weekly$IDStations==st),"AQ_pm10"]
y=as.numeric(y[[1]])
y=scale(y)[1:53]    #normalization on each single station ( 53 weeks)
covs=df_weekly[which(df_weekly$IDStations==st),vars_names]
k=dim(covs)[2]
covs=as.data.frame(covs)
covariates=scale(covs)     #normalization on each single station
data.out=as.matrix(outputRegress) # trasform the mcmc.list into a matrix,
data.out=data.frame(data.out)     # or, better, into a dataframe (easiest to handle in R)
attach(data.out)
n.chain=dim(data.out)[1]
#the betas
beta.post <- data.out[,0:k+1]   #posterior mean of the beta parameters
beta.bayes  <- apply(beta.post,2,"mean")
beta.bayes <- c(beta.bayes[k+1],beta.bayes[1:k])  #just to put beta0 in first position
bayesian_coefficients <- rbind(bayesian_coefficients,c(st,beta.bayes))
#linear (frequentist)
linear_model_classic= lm(y ~ .,data = data.frame(covs))
frequentist_coefficients <- rbind(frequentist_coefficients,c(st,linear_model_classic$coefficients))
# LASSO
#y = df_weekly$AQ_pm10[which(df_weekly$IDStations==st)]
x <- model.matrix(as.formula(formula),data.frame(covs))[,-1]
fit.lasso <- glmnet(x,y, lambda = lambda.grid)
cv.lasso <- cv.glmnet(x,y,lambda=lambda.grid)
bestlam.lasso <- cv.lasso$lambda.min
best_lasso_vect = c(best_lasso_vect,bestlam.lasso)
coef.lasso <- predict(fit.lasso, s=bestlam.lasso, type = 'coefficients')
coef.lasso =  data.frame(coef.lasso[,1])
coef.lasso$variables = rownames(coef.lasso)
colnames(coef.lasso)[1] = "coeff"
final_coeff = coef.lasso[which(abs(coef.lasso$coeff)>soglia),"variables"]
best_variables_lasso[which(best_variables_lasso$variables%in%final_coeff),"checked"] =
best_variables_lasso[which(best_variables_lasso$variables%in%final_coeff),"checked"] +1
detach(data.out)
}
#to give columns names
colnames(frequentist_coefficients)<-c('station',names(beta.bayes))
# It's the same code as before, but in a 'for' cycle
for (st in stations){
cat(crayon::red("Station",st,"\n"))
#covariates and target
y=df_weekly[which(df_weekly$IDStations==st),"AQ_pm10"]
y=as.numeric(y[[1]])
y=scale(y)[1:53]    #normalization on each single station ( 53 weeks)
covs=df_weekly[which(df_weekly$IDStations==st),vars_names]
k=dim(covs)[2]
covs=as.data.frame(covs)
covariates=scale(covs)     #normalization on each single station
#jitter to avoid nans in constant columns
for(i in 1:k){
if(sum(is.na(covariates[,i]))>0)
covs[,i]=jitter(covs[,i],0.01)
}
covs=scale(covs)
#build model
data = list(y=y,
x=covs,
n=dim(covs)[1],
k=k)
inits = function() {list(beta0=0,
beta=rep(0,k),
sigma=100) }
#run model
modelRegress=jags.model("LinearReg.bug",data=data,inits=inits,n.adapt=1000,n.chains=1)
update(modelRegress,n.iter=5000) #this is the burn-in period
#save betas and sigma
variable.names=c("beta0", "sigma","beta" ) #parameters
n.iter=50000
thin=10
outputRegress=coda.samples(model=modelRegress,variable.names=variable.names,n.iter=n.iter,thin=thin)
data.out=as.matrix(outputRegress) # trasform the mcmc.list into a matrix,
data.out=data.frame(data.out)     # or, better, into a dataframe (easiest to handle in R)
attach(data.out)
n.chain=dim(data.out)[1]
#the betas
beta.post <- data.out[,0:k+1]   #posterior mean of the beta parameters
beta.bayes  <- apply(beta.post,2,"mean")
beta.bayes <- c(beta.bayes[k+1],beta.bayes[1:k])  #just to put beta0 in first position
bayesian_coefficients <- rbind(bayesian_coefficients,c(st,beta.bayes))
detach(data.out)
}
bayesian_coefficients <- data.frame()
frequentist_coefficients <- data.frame()
vars_names = colnames(covs)
# stuff for lasso
formula <- " y ~ ."
lambda.grid <- 10^seq(5,-3,length=100)
soglia = 0.001
best_lasso_vect = c()
best_variables_lasso <- data.frame(variables = c("(Intercept)",vars_names), checked = rep(0,(length(vars_names)+1)))
best_variables_lasso
# It's the same code as before, but in a 'for' cycle
for (st in stations){
#covariates and target
y=df_weekly[which(df_weekly$IDStations==st),"AQ_pm10"]
y=as.numeric(y[[1]])
y=scale(y)[1:53]    #normalization on each single station ( 53 weeks)
covs=df_weekly[which(df_weekly$IDStations==st),vars_names]
k=dim(covs)[2]
covs=as.data.frame(covs)
covariates=scale(covs)     #normalization on each single station
data.out=as.matrix(outputRegress) # trasform the mcmc.list into a matrix,
data.out=data.frame(data.out)     # or, better, into a dataframe (easiest to handle in R)
#linear (frequentist)
linear_model_classic= lm(y ~ .,data = data.frame(covs))
frequentist_coefficients <- rbind(frequentist_coefficients,c(st,linear_model_classic$coefficients))
# LASSO
#y = df_weekly$AQ_pm10[which(df_weekly$IDStations==st)]
x <- model.matrix(as.formula(formula),data.frame(covs))[,-1]
fit.lasso <- glmnet(x,y, lambda = lambda.grid)
cv.lasso <- cv.glmnet(x,y,lambda=lambda.grid)
bestlam.lasso <- cv.lasso$lambda.min
best_lasso_vect = c(best_lasso_vect,bestlam.lasso)
coef.lasso <- predict(fit.lasso, s=bestlam.lasso, type = 'coefficients')
coef.lasso =  data.frame(coef.lasso[,1])
coef.lasso$variables = rownames(coef.lasso)
colnames(coef.lasso)[1] = "coeff"
final_coeff = coef.lasso[which(abs(coef.lasso$coeff)>soglia),"variables"]
best_variables_lasso[which(best_variables_lasso$variables%in%final_coeff),"checked"] =
best_variables_lasso[which(best_variables_lasso$variables%in%final_coeff),"checked"] +1
}
#to give columns names
colnames(frequentist_coefficients)<-c('station',names(beta.bayes))
head(frequentist_coefficients)
best_lasso_vect
best_variables_lasso
mean(best_lasso_vect)
var(best_lasso_vect)
mean(best_lasso_vect)
best_variables_lasso
hist(best_variables_lasso$checked)
hist(best_variables_lasso$checked,bins = 29)
hist(best_variables_lasso$checked,breaks = 29)
best_variables_lasso
chosen_variables = best_variables_lasso[best_variables_lasso$checked>10,"variables"]
chosen_variables
chosen_variables = best_variables_lasso[best_variables_lasso$checked>soglia_lasso,"variables"]
soglia_lasso = 10
soglia_lasso = 10
chosen_variables = best_variables_lasso[best_variables_lasso$checked>soglia_lasso,"variables"]
print(chosen_variables)
chosen_variables = best_variables_lasso[best_variables_lasso$checked>i,"variables"]
for(i in 1:105){
chosen_variables = best_variables_lasso[best_variables_lasso$checked>i,"variables"]
print(length(chosen_variables))
}
head(frequentist_coefficients)
mean(best_lasso_vect)
var(best_lasso_vect)
best_variables_lasso
hist(best_variables_lasso$checked,breaks = 29)
soglia_lasso = 10
num_variables_included = c()
for(i in 1:105){
chosen_variables = best_variables_lasso[best_variables_lasso$checked>i,"variables"]
num_variables_included = c(num_variables_included,length(chosen_variables))
}
plot(num_variables_included)
plot(num_variables_included,type="b")
plot(num_variables_included,type="l")
abline(h = num_variables_included_soglia,color= "red")
head(frequentist_coefficients)
mean(best_lasso_vect)
var(best_lasso_vect)
best_variables_lasso
hist(best_variables_lasso$checked,breaks = 29)
num_variables_included = c()
for(i in 1:105){
chosen_variables = best_variables_lasso[best_variables_lasso$checked>i,"variables"]
num_variables_included = c(num_variables_included,length(chosen_variables))
}
num_variables_included_soglia = 5
plot(num_variables_included,type="l")
abline(h = num_variables_included_soglia,color= "red")
plot(num_variables_included,type="l")
abline(h = num_variables_included_soglia,colour= "red")
plot(num_variables_included,type="l")
abline(h = num_variables_included_soglia,col= "red")
head(frequentist_coefficients)
mean(best_lasso_vect)
var(best_lasso_vect)
best_variables_lasso
hist(best_variables_lasso$checked,breaks = 29)
num_variables_included = c()
for(i in 1:105){
chosen_variables = best_variables_lasso[best_variables_lasso$checked>i,"variables"]
num_variables_included = c(num_variables_included,length(chosen_variables))
}
num_variables_included_soglia = 4
plot(num_variables_included,type="l")
abline(h = num_variables_included_soglia,col= "red")
head(frequentist_coefficients)
mean(best_lasso_vect)
var(best_lasso_vect)
best_variables_lasso
hist(best_variables_lasso$checked,breaks = 29)
num_variables_included = c()
for(i in 1:105){
chosen_variables = best_variables_lasso[best_variables_lasso$checked>i,"variables"]
num_variables_included = c(num_variables_included,length(chosen_variables))
}
num_variables_included_soglia = 4
soglia_lasso = 45
plot(num_variables_included,type="l")
abline(h = num_variables_included_soglia,col= "red")
abline(v = num_variables_included_soglia,col= "blue")
soglia_lasso = 45
plot(num_variables_included,type="l")
abline(h = num_variables_included_soglia,col= "red")
abline(v = soglia_lasso,col= "blue")
vars_names = best_variables_lasso[best_variables_lasso$checked>soglia_lasso,"variables"]
vars_names
# It's the same code as before, but in a 'for' cycle
for (st in stations){
cat(crayon::red("Station",st,"\n"))
#covariates and target
y=df_weekly[which(df_weekly$IDStations==st),"AQ_pm10"]
y=as.numeric(y[[1]])
y=scale(y)[1:53]    #normalization on each single station ( 53 weeks)
covs=df_weekly[which(df_weekly$IDStations==st),vars_names]
k=dim(covs)[2]
covs=as.data.frame(covs)
covariates=scale(covs)     #normalization on each single station
#jitter to avoid nans in constant columns
for(i in 1:k){
if(sum(is.na(covariates[,i]))>0)
covs[,i]=jitter(covs[,i],0.01)
}
covs=scale(covs)
#build model
data = list(y=y,
x=covs,
n=dim(covs)[1],
k=k)
inits = function() {list(beta0=0,
beta=rep(0,k),
sigma=100) }
#run model
modelRegress=jags.model("LinearReg.bug",data=data,inits=inits,n.adapt=1000,n.chains=1)
update(modelRegress,n.iter=5000) #this is the burn-in period
#save betas and sigma
variable.names=c("beta0", "sigma","beta" ) #parameters
n.iter=50000
thin=10
outputRegress=coda.samples(model=modelRegress,variable.names=variable.names,n.iter=n.iter,thin=thin)
data.out=as.matrix(outputRegress) # trasform the mcmc.list into a matrix,
data.out=data.frame(data.out)     # or, better, into a dataframe (easiest to handle in R)
attach(data.out)
n.chain=dim(data.out)[1]
#the betas
beta.post <- data.out[,0:k+1]   #posterior mean of the beta parameters
beta.bayes  <- apply(beta.post,2,"mean")
beta.bayes <- c(beta.bayes[k+1],beta.bayes[1:k])  #just to put beta0 in first position
bayesian_coefficients <- rbind(bayesian_coefficients,c(st,beta.bayes))
detach(data.out)
}
vars_names = vars_names[2:num_variables_included_soglia]
vars_names
# It's the same code as before, but in a 'for' cycle
for (st in stations){
cat(crayon::red("Station",st,"\n"))
#covariates and target
y=df_weekly[which(df_weekly$IDStations==st),"AQ_pm10"]
y=as.numeric(y[[1]])
y=scale(y)[1:53]    #normalization on each single station ( 53 weeks)
covs=df_weekly[which(df_weekly$IDStations==st),vars_names]
k=dim(covs)[2]
covs=as.data.frame(covs)
covariates=scale(covs)     #normalization on each single station
#jitter to avoid nans in constant columns
for(i in 1:k){
if(sum(is.na(covariates[,i]))>0)
covs[,i]=jitter(covs[,i],0.01)
}
covs=scale(covs)
#build model
data = list(y=y,
x=covs,
n=dim(covs)[1],
k=k)
inits = function() {list(beta0=0,
beta=rep(0,k),
sigma=100) }
#run model
modelRegress=jags.model("LinearReg.bug",data=data,inits=inits,n.adapt=1000,n.chains=1)
update(modelRegress,n.iter=5000) #this is the burn-in period
#save betas and sigma
variable.names=c("beta0", "sigma","beta" ) #parameters
n.iter=50000
thin=10
outputRegress=coda.samples(model=modelRegress,variable.names=variable.names,n.iter=n.iter,thin=thin)
data.out=as.matrix(outputRegress) # trasform the mcmc.list into a matrix,
data.out=data.frame(data.out)     # or, better, into a dataframe (easiest to handle in R)
attach(data.out)
n.chain=dim(data.out)[1]
#the betas
beta.post <- data.out[,0:k+1]   #posterior mean of the beta parameters
beta.bayes  <- apply(beta.post,2,"mean")
beta.bayes <- c(beta.bayes[k+1],beta.bayes[1:k])  #just to put beta0 in first position
bayesian_coefficients <- rbind(bayesian_coefficients,c(st,beta.bayes))
detach(data.out)
}
# It's the same code as before, but in a 'for' cycle
for (st in stations){
cat(crayon::red("Station",st,"\n"))
#covariates and target
y=df_weekly[which(df_weekly$IDStations==st),"AQ_pm10"]
y=as.numeric(y[[1]])
y=scale(y)[1:53]    #normalization on each single station ( 53 weeks)
covs=df_weekly[which(df_weekly$IDStations==st),vars_names]
k=dim(covs)[2]
covs=as.data.frame(covs)
covariates=scale(covs)     #normalization on each single station
#jitter to avoid nans in constant columns
for(i in 1:k){
if(sum(is.na(covariates[,i]))>0)
covs[,i]=jitter(covs[,i],0.01)
}
covs=scale(covs)
#build model
data = list(y=y,
x=covs,
n=dim(covs)[1],
k=k)
inits = function() {list(beta0=0,
beta=rep(0,k),
sigma=100) }
#run model
modelRegress=jags.model("LinearReg.bug",data=data,inits=inits,n.adapt=1000,n.chains=1)
update(modelRegress,n.iter=5000) #this is the burn-in period
#save betas and sigma
variable.names=c("beta0", "sigma","beta" ) #parameters
n.iter=50000
thin=10
outputRegress=coda.samples(model=modelRegress,variable.names=variable.names,n.iter=n.iter,thin=thin)
data.out=as.matrix(outputRegress) # trasform the mcmc.list into a matrix,
data.out=data.frame(data.out)     # or, better, into a dataframe (easiest to handle in R)
attach(data.out)
n.chain=dim(data.out)[1]
#the betas
beta.post <- data.out[,0:k+1]   #posterior mean of the beta parameters
beta.bayes  <- apply(beta.post,2,"mean")
beta.bayes <- c(beta.bayes[k+1],beta.bayes[1:k])  #just to put beta0 in first position
bayesian_coefficients <- rbind(bayesian_coefficients,c(st,beta.bayes))
detach(data.out)
}
#to give columns names
colnames(bayesian_coefficients)<-c('station',names(beta.bayes))
head(bayesian_coefficients)
dataframe_cut =df_weekly[which(df_weekly$IDStations==st),vars_names]
linear_model_classic <- lm(y~.,data = data.frame(dataframe_cut))
summary(linear_model_classic)
dataframe_cut =df_weekly[,vars_names]
linear_model_classic <- lm(y~.,data = data.frame(dataframe_cut))
dataframe_cut
dataframe_cut =df_weekly[,vars_names]
yy=df_weekly[,"AQ_pm10"]
linear_model_classic <- lm(yy~.,data = data.frame(dataframe_cut))
yy
dataframe_cut =df_weekly[,vars_names]
yy=df_weekly[,"AQ_pm10"]
linear_model_classic <- lm(yy~.,data = data.frame(dataframe_cut))
dataframe_cut =df_weekly[,vars_names]
yy=df_weekly[,"AQ_pm10"]
dataframe_cut$yy = yy
linear_model_classic <- lm(yy ~.,data = data.frame(dataframe_cut))
dataframe_cut =df_weekly[,vars_names]
yy=as.vector(df_weekly[,"AQ_pm10"])
dataframe_cut$yy = yy
linear_model_classic <- lm(yy ~.,data = data.frame(dataframe_cut))
linear_model_classic <- lm(yy ~.,data = data.frame(dataframe_cut))
dataframe_cut =df_weekly[,vars_names]
yy=df_weekly$AQ_pm10
linear_model_classic <- lm(yy ~.,data = data.frame(dataframe_cut))
summary(linear_model_classic)
covs=df_weekly[,3:39]
covs=covs[,-c(3,5,9,19,36)]    #not numerical
covs=covs[,-c(1,2,3,31)]   #lat, long, alti and land_use constant over same
dataframe_cut =covs
yy=df_weekly$AQ_pm10
linear_model_classic <- lm(yy ~.,data = data.frame(covs))
summary(linear_model_classic)
covs=df_weekly[,3:39]
covs=covs[,-c(3,5,9,19,36)]    #not numerical
y=df_weekly[,"AQ_pm10"]
y=as.numeric(y[[1]])
y=scale(y)
covs=df_weekly[,vars_names]
k=dim(covs)[2]
covs=as.data.frame(covs)
covariates=scale(covs)     #normalization on each single station
# LASSO
x <- model.matrix(as.formula(formula),data.frame(covs))[,-1]
fit.lasso <- glmnet(x,y, lambda = lambda.grid)
cv.lasso <- cv.glmnet(x,y,lambda=lambda.grid)
bestlam.lasso <- cv.lasso$lambda.min
coef.lasso <- predict(fit.lasso, s=bestlam.lasso, type = 'coefficients')
coef.lasso =  data.frame(coef.lasso[,1])
coef.lasso$variables = rownames(coef.lasso)
coef.lasso
covs=df_weekly[,3:39]
covs=covs[,-c(3,5,9,19,36)]    #not numerical
vars_names = colnames(covs)
y=df_weekly[,"AQ_pm10"]
y=as.numeric(y[[1]])
y=scale(y)
covs=df_weekly[,vars_names]
k=dim(covs)[2]
covs=as.data.frame(covs)
covariates=scale(covs)     #normalization on each single station
# LASSO
x <- model.matrix(as.formula(formula),data.frame(covs))[,-1]
fit.lasso <- glmnet(x,y, lambda = lambda.grid)
cv.lasso <- cv.glmnet(x,y,lambda=lambda.grid)
bestlam.lasso <- cv.lasso$lambda.min
coef.lasso <- predict(fit.lasso, s=bestlam.lasso, type = 'coefficients')
coef.lasso =  data.frame(coef.lasso[,1])
coef.lasso$variables = rownames(coef.lasso)
coef.lasso
y=df_weekly[,"AQ_pm10"]
y=as.numeric(y[[1]])
y=scale(y)
covs=df_weekly[,vars_names]
k=dim(covs)[2]
covs=as.data.frame(covs)
covariates=scale(covs)     #normalization on each single station
# LASSO
x <- model.matrix(as.formula(formula),data.frame(covs))[,-1]
fit.lasso <- glmnet(x,y, lambda = lambda.grid)
cv.lasso <- cv.glmnet(x,y,lambda=lambda.grid)
bestlam.lasso <- cv.lasso$lambda.min
coef.lasso <- predict(fit.lasso, s=bestlam.lasso, type = 'coefficients')
coef.lasso =  data.frame(coef.lasso[,1])
coef.lasso$variables = rownames(coef.lasso)
soglia = 0.001
final_coeff = coef.lasso[which(abs(coef.lasso$coeff)>soglia),"variables"]
y=df_weekly[,"AQ_pm10"]
y=as.numeric(y[[1]])
y=scale(y)
covs=df_weekly[,vars_names]
k=dim(covs)[2]
covs=as.data.frame(covs)
covariates=scale(covs)     #normalization on each single station
# LASSO
x <- model.matrix(as.formula(formula),data.frame(covs))[,-1]
fit.lasso <- glmnet(x,y, lambda = lambda.grid)
cv.lasso <- cv.glmnet(x,y,lambda=lambda.grid)
bestlam.lasso <- cv.lasso$lambda.min
coef.lasso <- predict(fit.lasso, s=bestlam.lasso, type = 'coefficients')
coef.lasso =  data.frame(coef.lasso[,1])
coef.lasso$variables = rownames(coef.lasso)
colnames(coef.lasso)[1] = "coeff"
soglia = 0.001
final_coeff = coef.lasso[which(abs(coef.lasso$coeff)>soglia),"variables"]
coef.lasso
final_coeff
soglia = 0.01
final_coeff = coef.lasso[which(abs(coef.lasso$coeff)>soglia),"variables"]
final_coeff
