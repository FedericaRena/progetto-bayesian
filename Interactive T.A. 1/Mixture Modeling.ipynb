{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfa8f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow_probability.substrates import numpy as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf3b65b-de60-4a2c-ace8-e6a7509c347e",
   "metadata": {},
   "source": [
    "## Group Members\n",
    "Federico Angelo Mor   \n",
    "Federica Rena   \n",
    "Giulia Mezzadri   \n",
    "Abylaikhan Orynbassar   \n",
    "Oswaldo Jesus Morales Lopez    \n",
    "Ettore Modina   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750369a",
   "metadata": {},
   "source": [
    "# Mixture Modelling (a.k.a. modelling heterogeneity ie clustering)\n",
    "$\\newcommand{\\iid}{\\stackrel{\\tiny\\mbox{iid}}{\\sim}}$\n",
    "In mixture models, we account for heterogeneity in the data by assuming that there is not \"one single\" data generating process, but actually H of them.\n",
    "\n",
    "Therefore, we expect to find H different subpopulations (== clusters) in our data, and that each subpopulation is homogeneous: it is suitably modeled by a density, typically from a parametric family.\n",
    "\n",
    "In its most general form, let $f_1(\\cdot), \\ldots, f_H(\\cdot)$ be $H$ probability density functions over a space $\\mathbb{Y}, \\mathcal{Y}$, $\\mathbf{w} = (w_1, \\ldots, w_H)$ a vector in the $H$-simplex; a mixture model assumes the following likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "    y_1, \\ldots, y_n \\mid \\mathbf{w}, \\{f_h\\}_{h=1}^H \\iid f^*(y) := \\sum_{h=1}^H w_h f_h(y)\n",
    "    \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "We refer to the $f_h$'s as the _components_ of the mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff5c81",
   "metadata": {},
   "source": [
    "$\\newcommand{\\ind}{\\stackrel{\\tiny\\mbox{ind}}{\\sim}}$\n",
    "The connection to clustering is made more explicit by introducing \"cluster assignment\" variables $c_i$, $i=1, \\ldots, n$ such that\n",
    "$$\n",
    "    P(c_i = h \\mid \\mathbf{w}) = w_h\n",
    "$$\n",
    "\n",
    "Then, the mixture likelihood can be restated as the following hierarchical model\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        y_i \\mid c_i = h, \\{f_j\\}_{j=1}^H & \\ind f_h \\\\\n",
    "        P(c_i = h \\mid \\mathbf{w}) = w_h\n",
    "    \\end{aligned}\n",
    "    \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "Usually, $\\{f_j\\}_{j=1}^H$ belong to the same parametric family (e.g., the normal distribution), and differ only for the specific values of the parameters (e.g., every component has a different mean/variance). But this might not always be the case.\n",
    "\n",
    "**REMARK: Properly justify all your answers!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c57e89",
   "metadata": {},
   "source": [
    "# Exercise 1: Gaussian Mixture Model\n",
    "$\\newcommand{\\iid}{\\stackrel{\\tiny\\mbox{iid}}{\\sim}}$\n",
    "Consider the dataset in the file `gestational_age_data.csv`, that contains the gestational age of more than two thousand children (in days).\n",
    "\n",
    "We can model the data using a mixture of three gaussian distributions.\n",
    "\n",
    "\\begin{equation*}\n",
    "    y_1, \\ldots, y_n \\mid \\mathbf{w}, \\mathbf{\\mu}, \\mathbf{\\sigma}^2 \\iid \\sum_{h=1}^3 w_h \\mathcal{N}(\\mu_h, \\sigma^2_h)\n",
    "\\end{equation*}\n",
    "\n",
    "1.1) Describe a simple prior for $\\mathbf{w}$.\n",
    "  \n",
    "1.2) Using the likelihood in (2), derive the full conditional for $\\mathbf{w}$.\n",
    "\n",
    "1.3) Assume $(\\mu_h, \\sigma_h)$'s to be i.i.d. and, for every $h$, assume a $\\mathcal{N}(\\mu_0, \\sigma^2_0)$ prior for $\\mu_h$ and a Uniform prior over $(l,u)$ for $\\sigma_h$, standard deviation of each mixture component. Specify the values of the prior hyperparameters $(\\mu_0, \\sigma^2_0, l, u)$ considering the empirical mean and standard deviation of the data. Is this prior conjugate?\n",
    "\n",
    "1.4) Derive a sampling strategy to sample from the full conditionals of $(\\mu_h, \\sigma_h)$, for a given $h$.  \n",
    "*(Hint: you may need a step of Metropolis-Hastings: use a Truncated Normal as the proposal distribution. Use the given `step_size` variable as the scale of the proposal)*\n",
    "\n",
    "1.5) Describe a hybrid Gibbs sampler algorithm that alternates between updating the $c_i$'s, the weights $\\mathbf w$ and the \"unique values\" $(\\mu_1, \\sigma_1^2), \\dots, (\\mu_H, \\sigma_H^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510bd92a-2325-4454-bf6b-b9fc9a2e5a70",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "1.1) We select a Dirichlet $\\mathbf{w} \\sim \\text{Dirichlet}(\\mathbf{\\alpha})$ with $\\alpha_1=\\alpha_2=\\alpha_3=2$.\n",
    "\n",
    "1.2) The posterior is a Dirichlet of params $\\alpha_i = \\alpha_i+n_i \\,\\forall i$; where $n_i$ are the numerosity of cluster i. See the image for the computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc546e98-2785-4d5d-bd21-554c996430cf",
   "metadata": {},
   "source": [
    "![foto1](./calcoli1.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9034d8-5a50-4556-8379-46d6a6324536",
   "metadata": {},
   "source": [
    "1.3) We chose mu0 = 274.032, sigma2_0 = 326.673 using the empirical mean and variance of the data.\n",
    "We chose l = 0, u = 200 to let each sigma_h to be \"free\" to vary according to the distribution of data in the different clusters. For example if a cluster is sparse his sigma_h can increase, while if a cluster is more dense the sigma_h can decrease.   \n",
    "The proposed priors are not conjugate. See again the image for computations.\n",
    "![](./calcoli3.jpeg)\n",
    "1.4)  \n",
    "\n",
    "1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c7ef65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feder\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "C:\\Users\\feder\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = np.loadtxt(\"gestational_age_data.csv\")\n",
    "np.random.shuffle(data)\n",
    "data = data[:1000]\n",
    "sns.histplot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec428c",
   "metadata": {},
   "source": [
    "### Prior Elicitation for $\\mu_h, \\sigma_h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1aa72e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical mean: 274.032; Empirical sd: 326.67297599999995\n",
      "We choose\n",
      "\n",
      "[mu0 = 274.032, sigma2_0 = 326.673]\n",
      "[l = 0, u = 100]\n"
     ]
    }
   ],
   "source": [
    "# We look at empirical mean and standard deviation\n",
    "muhat = np.mean(data)\n",
    "sdhat = np.var(data)\n",
    "print(\"Empirical mean: {0}; Empirical sd: {1}\".format(muhat, sdhat))\n",
    "\n",
    "# TODO: Prior elicitation for mu0, sigma2_0, l, u\n",
    "print(\"We choose\\n\")\n",
    "print(\"[mu0 = 274.032, sigma2_0 = 326.673]\")\n",
    "print(\"[l = 0, u = 200]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39163d6e-5658-4d33-8b28-4d033d3381cd",
   "metadata": {},
   "source": [
    "### Sampler implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b609cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_gibbs(data, cluster_allocs, uniq_vals, weights, step_size):\n",
    "    \"\"\"\n",
    "    Runs one iteration of the gibbs sampler\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: a numpy array of floats, contains the observations\n",
    "    cluster_allocs: a numpy array of integers, contains the c_i's\n",
    "        (same length of data)\n",
    "    uniq_vals: a numpy array of size [K, 2], contains (mean, variance)\n",
    "        for every component\n",
    "    weights: a numpy array of size [K], contains the weights of the\n",
    "        components\n",
    "    step_size: a double, contains the step size of the Random Walk Metropolis Hastings\n",
    "    \"\"\"\n",
    "\n",
    "    # Set n_clus\n",
    "    n_clus = len(weights)\n",
    "    # Update Unique Values\n",
    "    for h in range(n_clus):\n",
    "        clusdata = data[cluster_allocs == h]\n",
    "        if len(clusdata) == 0:\n",
    "            uniq_vals[h, :] = sample_uniq_vals_prior()\n",
    "        else:\n",
    "            uniq_vals[h, :] = sample_uniq_vals_fullcond(clusdata, uniq_vals[h, :], step_size)\n",
    "    # Update cluster allocs\n",
    "    cluster_allocs = update_cluster_allocs(data, weights, uniq_vals)\n",
    "    # Update weights\n",
    "    weights = update_weights(cluster_allocs, n_clus)\n",
    "    \n",
    "    return cluster_allocs, uniq_vals, weights\n",
    "\n",
    "\n",
    "\n",
    "def run_mcmc(data, niter=7500, nburn=2500, thin=5):\n",
    "\n",
    "    # Initialize chain\n",
    "    cluster_allocs = tfd.Categorical(probs=np.ones(3) / 3).sample(len(data))\n",
    "    weights = np.ones(3) / 3\n",
    "    step_size = 1e-1\n",
    "    # TODO: Complete by sampling from an appropriate distribution\n",
    "    uniq_vals = np.dstack([\n",
    "        tfd.??.sample(3),\n",
    "        (tfd.??.sample(3))**2])[0, :, :]\n",
    "    \n",
    "    # Prepare buffers\n",
    "    allocs_out = []\n",
    "    uniq_vals_out = []\n",
    "    weights_out = []\n",
    "    \n",
    "    # Sampler for loop\n",
    "    for i in range(niter):\n",
    "        # Sample\n",
    "        cluster_allocs, uniq_vals, weights = run_one_gibbs(\n",
    "            data, cluster_allocs, uniq_vals, weights, step_size)\n",
    "        # Save state\n",
    "        if i > nburn and i % thin == 0:\n",
    "            allocs_out.append(cluster_allocs)\n",
    "            uniq_vals_out.append(uniq_vals)\n",
    "            weights_out.append(weights)\n",
    "        # Print progress\n",
    "        if i % 10 == 9:\n",
    "            print(\"\\rIter {0} / {1}\".format(i+1, niter), flush=True, end=\" \")\n",
    "    \n",
    "    # Return chain\n",
    "    return allocs_out, uniq_vals_out, weights_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ac2952-3e32-4c11-81af-3205e50a8f4c",
   "metadata": {},
   "source": [
    "Each function in the following chunck implements the update of a portion of the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfd8c5-eccb-45ec-86f8-21dc9cf43b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_probability.substrates import numpy as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "def update_cluster_allocs(data, weights, uniq_vals):\n",
    "    # Hint: the update of each c_i can be done independently of\n",
    "    # the other c_j's (j != i) ---> Use numpy broadcasting /vectorizaiton\n",
    "    # to speed up the code\n",
    "    # Hint: you might need to evaluate the likelihood of each observation\n",
    "    # with parameters in each cluster. The following snippet returns a\n",
    "    # [n, H] matrix with [i, j]-entry equal to log_prob(y_i | \\mu_j, \\sigma^2_j)\n",
    "    \n",
    "    logprobs = tfd.Normal(\n",
    "        uniq_vals[:, 0], np.sqrt(uniq_vals[:, 1])).log_prob(data[:, np.newaxis])\n",
    "    \n",
    "    # TODO: compute probs\n",
    "   \n",
    "    return tfd.Categorical(probs=probs).sample()\n",
    "\n",
    "\n",
    "def update_weights(cluster_allocs, n_clus):\n",
    "    # Hint: you might need to count how many observations are in each cluster\n",
    "    # you can use the following code\n",
    "    # n_by_clus = np.sum(cluster_allocs == np.arange(3)[:, np.newaxis], axis=1)\n",
    "    \n",
    "    # TODO: compute post_params\n",
    "    post_params = None\n",
    "    return tfd.DISTRIBUTION(post_params.astype(float)).sample()\n",
    "\n",
    "\n",
    "def sample_uniq_vals_prior():\n",
    "    mu = tfd.??.sample()\n",
    "    var = (tfd.??.sample()) ** 2\n",
    "    return np.array([mu, var])\n",
    "\n",
    "\n",
    "def sample_uniq_vals_fullcond(clusdata, curr_unique_vals, step_size):\n",
    "    # Get current values\n",
    "    curr_mu = curr_unique_vals[0]\n",
    "    curr_sigma = curr_unique_vals[1]\n",
    "\n",
    "    # TODO: Sample mu\n",
    "    mu = tfd.??.sample()\n",
    "\n",
    "    # TODO: Sample sigma\n",
    "    \n",
    "    def target_lpdf(state, clusdata, loc):\n",
    "        target = ??\n",
    "        return target\n",
    "    \n",
    "    def prop_lpdf(state, loc):\n",
    "        return tfd.??.log_prob(state)\n",
    "    \n",
    "    # TODO: Define curr_state and prop_state\n",
    "    curr_state = np.sqrt(curr_sigma)\n",
    "    prop_state = ??\n",
    "    \n",
    "    # TODO: Compute acceptance rate\n",
    "    log_arate = ??\n",
    "    \n",
    "    # Test if prop_state is accepted\n",
    "    if np.log(tfd.Uniform(0,1).sample()) < log_arate:\n",
    "        var = prop_state ** 2\n",
    "    else:\n",
    "        var = curr_state ** 2\n",
    "\n",
    "    # Return [mu, var]\n",
    "    return np.array([mu, var])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ed096-407c-470c-aefd-fec3e4d876d3",
   "metadata": {},
   "source": [
    "### Run the Sampler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e547d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "allocs_out, uniq_vals_out, weights_out = run_mcmc(data, niter=1500, nburn=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052bf8bf-b0ac-4870-adbd-6bac4ff4b859",
   "metadata": {},
   "source": [
    "Let's see an example of clustering by plotting the last values of `allocs_out` against the data histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a97d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data, density=True, alpha=0.1)\n",
    "for h in range(3):\n",
    "    currd = data[allocs_out[-1] == h]\n",
    "    plt.scatter(currd, np.zeros_like(currd) + 0.001 * (h+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e49229-58e9-473a-a8ed-da39f6fd9de7",
   "metadata": {},
   "source": [
    "## Hint\n",
    "If you want to do Exs 1 & 2 in parallel, you can simulate a fake Markov Chain that mimics the output of Ex1.\n",
    "\n",
    "For $j=1, \\ldots, M$ sample: ($\\mathcal{IG}$ is the invGamma)\n",
    "\n",
    "\\begin{align}\n",
    "    &\\mu_1^{(j)} \\sim \\mathcal{N}(-5, 0.5), \\quad \\mu_2^{(j)} \\sim \\mathcal{N}(5, 0.5); \\\\[2pt]\n",
    "    &\\sigma^{2, (j)}_1 \\sim \\mathcal{IG}(3,3), \\quad \\sigma^{2, (j)}_2 \\sim \\mathcal{IG}(3,3); \\\\[2pt]\n",
    "    &c_i^{(j)} \\sim \\text{Categorical(0.9, 0.1)} \\text{, for } i = 1, \\dots, 50; \\\\[2pt]\n",
    "    &c_i^{(j)} \\sim \\text{Categorical(0.1, 0.9)} \\text{, for } i = 51, \\ldots, 100; \\\\[2pt]\n",
    "    &\\mathbf{w}^{(j)} \\sim \\text{Dirichlet}(5, 5).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "99ce8971-2806-4bd8-8a32-6a3acfb9e2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MU\n",
      " [[-4.6143845   4.23679767]\n",
      " [-5.44580774  5.07336706]\n",
      " [-5.21707591  5.66549902]\n",
      " [-4.94503787  5.86032168]\n",
      " [-4.91766224  5.73281863]\n",
      " [-4.89952975  5.56164345]\n",
      " [-5.96018414  5.58001378]\n",
      " [-5.38787825  4.63602775]\n",
      " [-5.09763525  6.02010433]\n",
      " [-4.87444392  4.97100775]]\n",
      "SIGMA\n",
      " [[0.59381761 0.90129654]\n",
      " [1.04172107 0.38135514]\n",
      " [1.35995951 3.71428068]\n",
      " [0.74168445 0.50020527]\n",
      " [2.03039932 5.21239295]\n",
      " [0.66526128 1.53833629]\n",
      " [1.19045054 1.74789792]\n",
      " [0.93724679 1.13003046]\n",
      " [1.75048257 1.24448685]\n",
      " [1.88190818 0.55917219]]\n",
      "C (labels 1 and 2)\n",
      " [[2. 1. 2. 2. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 2. 1. 2. 1. 2. 1. 2. 2. 2.\n",
      "  2. 1. 1. 2. 1. 2. 2. 2. 1. 2. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1.\n",
      "  2. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 1. 2. 1. 2. 1. 1. 2. 1. 2. 2. 1. 1. 2.\n",
      "  2. 1. 1. 1. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 1. 2. 1. 2. 2. 2. 1. 1.\n",
      "  1. 2. 2. 1.]\n",
      " [1. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 1. 1. 2. 1. 2. 2. 2. 2. 2. 1. 1. 2.\n",
      "  2. 2. 1. 1. 1. 2. 2. 1. 2. 2. 2. 1. 2. 1. 2. 1. 1. 2. 1. 1. 2. 2. 1. 2.\n",
      "  1. 2. 1. 2. 2. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 1. 1. 2. 2. 2. 1.\n",
      "  2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 2. 2. 1. 1. 2. 2. 1.\n",
      "  2. 2. 1. 2.]\n",
      " [1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 2. 2. 1. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1. 2.\n",
      "  2. 2. 2. 1. 2. 1. 2. 2. 1. 2. 2. 1. 2. 1. 2. 2. 1. 2. 1. 1. 1. 1. 2. 2.\n",
      "  2. 1. 1. 2. 1. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 1.\n",
      "  2. 1. 2. 2. 2. 1. 1. 2. 2. 1. 1. 1. 2. 1. 2. 1. 2. 1. 1. 2. 1. 1. 1. 2.\n",
      "  2. 1. 1. 2.]\n",
      " [1. 1. 2. 2. 2. 1. 2. 1. 1. 1. 2. 2. 1. 1. 2. 2. 2. 1. 2. 2. 1. 1. 1. 2.\n",
      "  1. 1. 2. 2. 2. 2. 1. 2. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1. 2. 2. 1. 2. 1. 1.\n",
      "  2. 2. 1. 2. 1. 2. 1. 1. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2.\n",
      "  1. 2. 1. 1. 2. 2. 2. 1. 1. 1. 2. 2. 1. 1. 2. 2. 1. 1. 1. 2. 2. 2. 2. 2.\n",
      "  1. 1. 1. 2.]\n",
      " [2. 1. 2. 2. 1. 2. 1. 2. 2. 2. 1. 2. 1. 1. 1. 2. 1. 1. 2. 2. 1. 2. 2. 2.\n",
      "  1. 1. 1. 1. 1. 2. 2. 2. 2. 1. 2. 2. 1. 2. 2. 2. 1. 2. 2. 2. 1. 1. 1. 2.\n",
      "  2. 2. 2. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 2. 1. 1. 1. 1. 2. 2. 1. 2. 1.\n",
      "  1. 1. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 2. 1. 2. 2. 1. 1. 2. 1.\n",
      "  1. 2. 1. 2.]\n",
      " [1. 2. 1. 1. 2. 2. 1. 1. 2. 1. 1. 2. 2. 1. 1. 1. 2. 1. 2. 2. 2. 2. 2. 1.\n",
      "  2. 2. 1. 2. 1. 1. 2. 1. 1. 1. 1. 2. 1. 2. 2. 2. 2. 1. 1. 2. 2. 1. 1. 2.\n",
      "  2. 1. 1. 1. 2. 1. 2. 1. 1. 1. 2. 2. 1. 1. 2. 2. 2. 2. 1. 2. 1. 1. 2. 2.\n",
      "  2. 1. 2. 2. 2. 1. 2. 1. 2. 1. 1. 1. 2. 1. 1. 2. 2. 1. 1. 1. 1. 2. 2. 2.\n",
      "  1. 1. 1. 1.]\n",
      " [1. 2. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 2. 1. 2. 1. 2. 2. 1. 2. 2. 1. 1. 1.\n",
      "  2. 1. 1. 1. 2. 1. 2. 2. 1. 1. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      "  2. 1. 2. 1. 1. 1. 2. 2. 1. 2. 1. 1. 1. 2. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2.\n",
      "  1. 1. 1. 2. 2. 1. 1. 1. 2. 2. 2. 1. 2. 2. 1. 1. 1. 2. 1. 1. 2. 2. 2. 2.\n",
      "  1. 1. 2. 1.]\n",
      " [2. 2. 1. 2. 1. 1. 1. 2. 2. 1. 2. 1. 2. 2. 1. 2. 1. 1. 1. 2. 1. 1. 2. 1.\n",
      "  2. 1. 1. 1. 2. 1. 1. 2. 2. 2. 1. 1. 1. 1. 1. 2. 2. 1. 2. 1. 2. 1. 2. 1.\n",
      "  1. 2. 2. 1. 1. 1. 2. 2. 1. 2. 1. 1. 1. 1. 1. 2. 2. 1. 2. 2. 1. 1. 1. 2.\n",
      "  2. 2. 1. 1. 1. 2. 2. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 2. 1. 1. 2. 1. 2.\n",
      "  1. 2. 2. 2.]\n",
      " [1. 2. 2. 2. 1. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2. 1. 1. 2. 2. 1. 1. 2. 1. 1.\n",
      "  2. 2. 1. 1. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 1. 1. 2. 1. 1. 2. 2. 1. 1. 2.\n",
      "  2. 1. 2. 2. 1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 2. 2. 2. 1. 2. 2. 1. 2.\n",
      "  1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 2. 1. 1. 2. 1. 2. 2. 1. 1. 1. 2. 2. 2. 2.\n",
      "  1. 1. 2. 2.]\n",
      " [1. 1. 2. 2. 2. 1. 1. 2. 1. 1. 2. 2. 2. 1. 2. 1. 2. 2. 1. 1. 1. 2. 2. 2.\n",
      "  2. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 2. 2. 1. 2. 1. 2. 2. 2. 2. 1.\n",
      "  2. 1. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 1. 1. 2. 1. 1. 1. 2. 2. 1. 1. 1. 2.\n",
      "  1. 1. 1. 2. 1. 2. 2. 2. 1. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 2. 1. 2. 2. 2.\n",
      "  2. 2. 1. 2.]]\n",
      "W\n",
      " [[0.43109343 0.56890657]\n",
      " [0.66279022 0.33720978]\n",
      " [0.66353596 0.33646404]\n",
      " [0.74566517 0.25433483]\n",
      " [0.38001632 0.61998368]\n",
      " [0.56408763 0.43591237]\n",
      " [0.34926622 0.65073378]\n",
      " [0.62470612 0.37529388]\n",
      " [0.70764927 0.29235073]\n",
      " [0.66181814 0.33818186]]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import invgamma\n",
    "from scipy.stats import dirichlet\n",
    "\n",
    "M=100\n",
    "nobs=100 # change to 100 for the actual test\n",
    "\n",
    "MU = np.zeros((M,2))\n",
    "SIGMA = np.zeros((M,2))\n",
    "C = np.zeros((M,nobs))\n",
    "W = np.zeros((M,2))\n",
    "\n",
    "for i in range(0,M):\n",
    "    MU[i,0] = np.random.normal(-5,0.5)\n",
    "    MU[i,1] = np.random.normal(5,0.5)\n",
    "    \n",
    "    SIGMA[i,0] = invgamma(3, scale=3).rvs()\n",
    "    SIGMA[i,1] = invgamma(3, scale=3).rvs()\n",
    "    \n",
    "    C[i,0:nobs//2] = np.random.choice(['1','2'],nobs//2,[0.9,0.1])\n",
    "    C[i,nobs//2:] = np.random.choice(['1','2'],nobs//2,[0.1,0.9])\n",
    "\n",
    "    W[i,0],W[i,1] = np.random.dirichlet([5,5])\n",
    "\n",
    "\n",
    "print(\"MU\\n\",MU[0:10,:])\n",
    "print(\"SIGMA\\n\",SIGMA[0:10,:])\n",
    "print(\"C (labels 1 and 2)\\n\",C[0:10,:])\n",
    "print(\"W\\n\",W[0:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8082580",
   "metadata": {},
   "source": [
    "# Exercise 2: Posterior inference in Mixture Models\n",
    "\n",
    "Mixture models are subject to the so called \"label switching\", which causes non-identifiability for all the parameters.\n",
    "\n",
    "Label switching means that permuting the \"labels\" {1, 2, ..., K} of the cluster yields the same exact likelihood.\n",
    "Hence, it does not make sense to talk about \"first\", \"second\" and \"third\" cluster.\n",
    "Moreover, also interpretation of the cluster parameters is tricky: we cannot say that\n",
    "\n",
    "$$\n",
    "    \\frac{1}{M} \\sum_{j=1}^M \\mu^{(j)}_1\n",
    "$$\n",
    "\n",
    "is an estimate of parameter $\\mu_1$. Also, it does not make sense to take averages of the cluster allocation labels.\n",
    "\n",
    "There are several ways to solve this non-identifiability issue, we will focus here on a decision-theoretic approach.\n",
    "\n",
    "\n",
    "2.1) Implement a function that finds the clustering, among the ones visited in the MCMC chain, that minimizes Binder's loss function:\n",
    "\n",
    "$$\n",
    "    \\sum_{i=1}^n \\sum_{j=1}^n (\\mathbb I [c_i = c_j] - p_{ij})^2\n",
    "$$\n",
    "where $p_{ij}$ is the posterior probability that observations $i$ and $j$ belong to the same cluster.\n",
    "\n",
    "Note that $p_{ij}$ can be easily estimated from the MCMC chains!\n",
    "\n",
    "2.2) Given the estimated clustering $\\mathbf{c}^*$, estimate the parameters in each cluster. Let $c^*_h$ denote the index-set of the $h$--th cluster (e.g., $c^*_1 = \\{1, 3, 10\\}$, $c^*_2 = \\{0, 2, 4, 5, 6\\}$ ...)\n",
    "Then\n",
    "$$\n",
    "    \\hat \\mu_h = \\frac{1}{M} \\sum_{j=1}^M \\frac{1}{\\# c^*_h} \\sum_{i \\in c^*_h} \\mu^{(j)}_{c_{i}^{(j)}}\n",
    "$$\n",
    "where $\\mu^{(j)}_\\ell$ is the mean of the $\\ell$--th cluster at the $j$--th MCMC iteration.\n",
    "\n",
    "Similarly for the variance $\\hat \\sigma_h^2$.\n",
    "\n",
    "2.3) Comment on the posterior inference obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173a9811-7be7-4fcf-a310-d50ed78021b1",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b98fbd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_psm(clus_alloc_chain):\n",
    "    \"\"\"\n",
    "    Returns the posterior similarity matrix, i.e.\n",
    "        out[i, j] = P(c_i == c_j)\n",
    "    for each pair of observations\n",
    "    \"\"\"\n",
    "    c_chain = np.vstack(clus_alloc_chain)\n",
    "    out = np.zeros((c_chain.shape[1], c_chain.shape[1]))\n",
    "    \n",
    "    # TODO: fill out!\n",
    "    for i in range(c_chain.shape[1]):\n",
    "        for j in range(i, c_chain.shape[1]):\n",
    "            prob_same_cluster = np.mean(c_chain[:, i] == c_chain[:, j])\n",
    "            out[i, j] = prob_same_cluster\n",
    "            out[j, i] = prob_same_cluster\n",
    "    \n",
    "    return out + out.T + np.eye(out.shape[0])\n",
    "\n",
    "\n",
    "def minbinder_sample(clus_alloc_chain, psm):\n",
    "    losses = np.zeros(len(clus_alloc_chain))\n",
    "    c_chain = np.vstack(clus_alloc_chain)\n",
    "    \n",
    "    # You can either cycle through the iterations, or \n",
    "    # cycle through the entries in the PSM [i, j]\n",
    "    # and vectorize the same operation for each iteration!\n",
    "    \n",
    "    # TODO: compute the losses!\n",
    "    for i in range(len(clus_alloc_chain)):\n",
    "      loss = 0.0\n",
    "      for j in range(c_chain.shape[1]):\n",
    "          for k in range(c_chain.shape[1]):\n",
    "              if j != k:\n",
    "                  loss += 1 - (psm[j, k] * (c_chain[i, j] == c_chain[i, k]) + (1 - psm[j, k]) * (c_chain[i, j] != c_chain[i, k]))\n",
    "\n",
    "    losses[i] = loss\n",
    "\n",
    "    \n",
    "    best_iter = np.argmin(losses)\n",
    "    return clus_alloc_chain[best_iter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aec7b09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "allocs_out = C\n",
    "print(np.shape(C))\n",
    "psm = get_psm(allocs_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0440dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clus = minbinder_sample(allocs_out, psm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "85c444c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 1000 but corresponding boolean dimension is 100",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(data, density\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     currd \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_clus\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mscatter(currd, np\u001b[38;5;241m.\u001b[39mzeros_like(currd) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.001\u001b[39m \u001b[38;5;241m*\u001b[39m (h\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 1000 but corresponding boolean dimension is 100"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data, density=True, alpha=0.3)\n",
    "for h in range(3):\n",
    "    currd = data[best_clus == h]\n",
    "    plt.scatter(currd, np.zeros_like(currd) + 0.001 * (h+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015eb95-ab4d-415d-869f-d9a84eb62008",
   "metadata": {},
   "source": [
    "2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a58ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_vals_given_clus(unique_vals_chain, clus_alloc_chain, best_clus):\n",
    "    c_allocs = np.stack(clus_alloc_chain)\n",
    "    uniq_vals = np.stack(unique_vals_chain)\n",
    "    means = uniq_vals[:, :, 0]\n",
    "    variances = uniq_vals[:, :, 1]\n",
    "    out = []\n",
    "    \n",
    "    for h in range(len(np.unique(best_clus))):\n",
    "        data_idx = np.where(best_clus == h)[0]\n",
    "        uniq_vals_idx = c_allocs[:, data_idx] # -> Matrix [n_iter x n_data_in_clus]\n",
    "        means_by_iter = np.empty((c_allocs.shape[0], len(data_idx)))\n",
    "        vars_by_iter = np.empty_like(means_by_iter)\n",
    "        for i in range(c_allocs.shape[0]):\n",
    "            means_by_iter[i, :] = ??\n",
    "            vars_by_iter[i, :] = ??\n",
    "\n",
    "        avg_mean_by_iter = ??\n",
    "        avg_var_by_iter = ??\n",
    "        \n",
    "        muhat = np.mean(avg_mean_by_iter)\n",
    "        sigsqhat = np.mean(avg_var_by_iter)\n",
    "        out.append(np.array([muhat, sigsqhat]))\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "clus_vals = unique_vals_given_clus(uniq_vals_out, allocs_out, best_clus)\n",
    "clus_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e136ca4-8539-4fdb-b54c-fd089ee94d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.sum(best_clus == np.arange(3)[:, np.newaxis], axis=1) / len(data)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data, density=True, alpha=0.1, bins=30)\n",
    "xx = np.linspace(np.min(data), np.max(data), 1000)\n",
    "out_dens = np.zeros_like(xx)\n",
    "for i in range(3):\n",
    "    comp_dens = tfd.Normal(clus_vals[i][0], np.sqrt(clus_vals[i][1])).prob(xx)\n",
    "    out_dens += weights[i] * comp_dens\n",
    "    plt.fill_between(xx, np.zeros_like(xx), weights[i] * comp_dens, alpha=0.6)\n",
    "\n",
    "plt.plot(xx, out_dens, c=\"black\", linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca74a0d0-2bd7-419d-8bf3-a549b7b63f57",
   "metadata": {},
   "source": [
    "2.3) **Comments on the inference obtained**  \n",
    "The three clusters differ in...  \n",
    "Their weights are ...  \n",
    "Hence we can conclude that the population ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9f91c",
   "metadata": {},
   "source": [
    "# Exercise 3: Latent Dirichlet Allocation\n",
    "\n",
    "We now move to one of the most fundamental papers in Machine Learning / Text Mining: Latent Dirichlet Allocations by [Blei et al (2003)](https://dl.acm.org/doi/10.5555/944919.944937).\n",
    "\n",
    "The goal is topic modelling given a corpus (= collection of documents)\n",
    "\n",
    "Denote with $\\mathbf y_i = \\{y_{i1}, \\ldots, y_{i N_i}\\}$ the $i$--th document. Let $|V|$ be the size of the vocabulary (number of unique words) and $K$ the number of topics.\n",
    "\n",
    "For each topic $k$, we assume a different distribution over the vocabulary $\\beta_k = (\\beta_{k,1}, \\ldots \\beta_{k, |V|})$.\n",
    "\n",
    "Moreover, in each document more than one topic can be discussed. Denote with $w_i = (w_{i, 1}, \\ldots, w_{i, K})$ the weight of each topic in the $i$--th document.\n",
    "\n",
    "Then LDA assumes\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    P(y_{i, j} = v \\mid \\{\\beta_k\\}_{k=1}^J, w_i) &= \\sum_{k=1}^K w_{i, k} \\beta_{k, v}, \\quad v=1, \\ldots, |V| \\\\\n",
    "    \\beta_k &\\sim \\text{Dirichlet}(\\alpha_1, \\ldots, \\alpha_{|V|}) \\\\\n",
    "    w_i & \\sim \\text{Dirichlet}(0.5, \\ldots, 0.5)\n",
    "\\end{aligned}\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "3.1) Rewrite the likelihood introducing suitable auxiliary variables $t_{ij}$ so that $w_i$ does appear on the right hand side of the first equation in (3)\n",
    "\n",
    "3.2) Specify a prior for the $\\beta_j$'s such that, a priori, the expected value of $\\beta_{j, v}$ is proportional to the number of times word $v$ appears in the corpus. What about the variance?\n",
    "\n",
    "3.3) Implement a Gibbs sampler to perform posterior inference. Instead of identifying the mixture parameters, consider only the last iteration of the MCMC. What can we say about the topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066864e8-664f-466c-85c5-91e0d9b20c8a",
   "metadata": {},
   "source": [
    "## Answers (trace)\n",
    "\n",
    "3.1) We introduce $t_{ij} \\sim \\text{TODO}$ (one for each $y_{ij}$) so that\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    P(y_{ij} = v \\mid \\{\\beta_k\\}, t_{ij} = h) &= \\beta_{h, v} \\\\\n",
    "    P(t_{ij} = h \\mid \\pi_i) &= \\text{TODO}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "3.2) We fix the expected value as ...  \n",
    "To control the variance, call $d_v =  \\sum_{i=1}^D \\sum_{j=1}^{N_i} \\mathbb{I}[y_{ij} = v]$, so that $\\alpha_v = \\kappa d_v$.\n",
    "\n",
    "3.3) The joint model is\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    \\mathcal{L}(\\{y_{ij}\\}, \\{\\beta_k\\}, \\{w_i\\}, \\{t_{ij}\\}) &= \\prod_{i=1}^D \\prod_{j=1}^{N_i} w_{i, t_{ij}} \\prod_{v=1}^{|V|} \\beta_{t_{ij}, v}^{\\mathbb{I}[y_{ij} = v]} \\\\\n",
    "    & \\times \\prod_{k=1}^K \\frac{1}{B(\\mathbf \\alpha)} \\prod_{v=1}^{|V|} \\beta_{k, v}^{\\alpha_v - 1} \\mathbb{I}[\\beta_k \\in S^{|V|}] \\\\\n",
    "    & \\times \\prod_{i=1}^D \\frac{1}{B(\\mathbf{0.5})} \\prod_{k=1}^{K} w_{i, k}^{0.5 - 1} \\mathbb{I}[w_i \\in S^{K}]\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Then it is easy to see that\n",
    "\n",
    "1) $\\mathcal{L}(w_i \\mid \\cdots) \\propto \\prod_{k=1}^{K} \\text{TODO}$\n",
    "\n",
    "   *(Hint: rewrite $\\prod_{j=1}^{N_i} w_{i, t_{ij}} = \\prod_{k=1}^K w_{i, k}^{n_{i, k}}$, where $n_{i, k} = ??$)*  \n",
    "   Finally, conclude that $w_i \\mid \\cdots \\sim \\text{TODO}$\n",
    "\n",
    "\n",
    "2) $ \\mathcal{L}(\\beta_k \\mid \\cdots) \\propto \\text{TODO} $\n",
    "   \n",
    "   Hence $\\beta_k \\mid \\cdots \\sim \\text{TODO}$\n",
    "\n",
    "   \n",
    "3) If $y_{ij} = v^*$, $P(t_{ij} = k \\mid \\cdots) \\propto w_{ik} \\beta_{k, v^*}, \\quad k=1, \\ldots, K$\n",
    "   \n",
    "   Hence, $t_{ij}$ is categorically-distributed with probabilities proportional to $w_{ik} \\beta_{k, v^*}$. We can sample $t_{ij}$ from a Categorical distribution after having computed the unnormalized probabilities for each $k$ and having normalized them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f6f452-0477-4085-a747-e83b40e6a26f",
   "metadata": {},
   "source": [
    "## Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d2f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install scikit-learn if not available\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups()[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903b1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b52b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some preprocessing.\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"Removes digits and weird punctuation\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\__', '', text)\n",
    "    text = re.sub(r'\\___', '', text)\n",
    "    return text\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    preprocessor=preprocess_text,\n",
    "    stop_words=\"english\",\n",
    "    strip_accents=\"unicode\",\n",
    "    max_df=0.95,\n",
    "    min_df=0.01,\n",
    "    max_features=500)\n",
    "\n",
    "vectorizer.fit(data)\n",
    "print(vectorizer.get_feature_names_out()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a620e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = vectorizer.build_analyzer()\n",
    "docs = []\n",
    "for doc in data:\n",
    "    tokens = analyzer(doc)\n",
    "    idxs = np.array(list(filter(\n",
    "        lambda x: x, \n",
    "        [vectorizer.vocabulary_.get(tok, None) for tok in tokens])))\n",
    "    docs.append(idxs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc85eb5-18ef-4c2d-a0a1-0fc4239cbdfc",
   "metadata": {},
   "source": [
    "**`docs` is your final dataset, work only with it!** `docs` is a list where every entry represents a document. A document is represented as a list of indexes (each work represents a word)\n",
    "\n",
    "### Sampler implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f6e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampler quantities\n",
    "n_topics = 20\n",
    "voc_len = len(vectorizer.get_feature_names_out())\n",
    "alpha = np.ones(voc_len) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a5a5f7-2080-4dd7-980d-61632d596504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_gibbs_sampler(docs):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs: a list of np.arrays of integers, each represnts the \n",
    "        words in a document\n",
    "        \n",
    "    word_topics are the t_{ij}'s\n",
    "    voc_weights are the \\beta_k's\n",
    "    doc_weights are the w_i's\n",
    "    \"\"\"\n",
    "    \n",
    "    word_topics = [\n",
    "        tfd.Categorical(probs=np.ones(n_topics)).sample(len(x)) for x in docs]\n",
    "    voc_weights = tfd.Dirichlet(alpha).sample(n_topics)\n",
    "    doc_weights = tfd.Dirichlet(np.ones(n_topics) * 0.5).sample(len(docs))\n",
    "    \n",
    "    for i in range(100):\n",
    "        print(\"\\rIter {0} / {1}\".format(i+1, 100), flush=True, end=\" \")\n",
    "        word_topics, doc_weights, voc_weights = run_one_lda(\n",
    "            docs, word_topics, doc_weights, voc_weights)\n",
    "    return word_topics, doc_weights, voc_weights\n",
    "\n",
    "\n",
    "def run_one_lda(docs, word_topics, doc_weights, voc_weights):    \n",
    "    word_topics = update_word_topics(doc_weights, voc_weights, docs)\n",
    "    doc_weights = update_doc_weights(word_topics)\n",
    "    voc_weights = update_voc_weights(docs, word_topics)\n",
    "    return word_topics, doc_weights, voc_weights\n",
    "\n",
    "\n",
    "def update_voc_weights(docs, word_topics):\n",
    "    \"\"\"Updates \\beta_k for every k\"\"\"\n",
    "    out = np.zeros((n_topics, voc_len))\n",
    "    for k in range(n_topics):\n",
    "        # Get all the words whose topic is k (i.e. t_ij = k)\n",
    "        # Kept words is a np.array of integers with the indexes/words\n",
    "        # of all y_{ij} for which t_{ij} = k\n",
    "        mask = [np.where(x == k) for x in word_topics]\n",
    "        kept_words = np.concatenate([x[y] for x, y in zip(docs, mask)])\n",
    "        \n",
    "        # Returns the unique values in kept_words and their counts\n",
    "        # For instance if kept_words = [1, 1, 1, 0, 5, 4]\n",
    "        # --> idxs = [0, 1, 4, 5]\n",
    "        # --> counts = [1, 3, 1, 1]\n",
    "        idxs, counts = np.unique(kept_words, return_counts=True)\n",
    "        \n",
    "        dir_param = ???\n",
    "        out[k] = tfd.DISTRIBUTION(dir_param).sample()\n",
    "    return out\n",
    "\n",
    "\n",
    "def update_doc_weights(word_topics):\n",
    "    \"\"\"Updates w_i for every i\"\"\"\n",
    "    out = np.zeros((len(docs), n_topics))\n",
    "    for doc_ind, word_topic_in_doc in enumerate(word_topics):\n",
    "        # word_topic_in_doc is the doc_ind-th row of word_topics\n",
    "        \n",
    "        # cnt returns how many times each topic appears in each document\n",
    "        # i.e. cnt[k] is the number of words ind the doc_ind-th document\n",
    "        # that have topic k\n",
    "        cnt = np.sum(\n",
    "            word_topic_in_doc == np.arange(n_topics)[:, np.newaxis], axis=1)\n",
    "        \n",
    "        dir_param = ???\n",
    "        out[doc_ind] = tfd.DISTRIBUTION(dir_param).sample()\n",
    "    return out\n",
    "\n",
    "\n",
    "def update_word_topics(doc_weights, voc_weights, docs):\n",
    "    out = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        # computes an [num_words x K] matrix with entries\n",
    "        # probas[l, m] beta_{m, v_l} * w_{i, m}\n",
    "        # where v_l is the word in position 'l'\n",
    "        # --> for each word (by row) represents the unnormalized probability\n",
    "        #     of each topic\n",
    "        probas = doc_weights[i][np.newaxis, :] * voc_weights[:, doc].T\n",
    "        \n",
    "        # normalize it approriately\n",
    "        probas = ???\n",
    "        out.append(tfd.Categorical(probs=probas).sample())\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590319b-124b-4bb8-a0a9-ff7db51d6db0",
   "metadata": {},
   "source": [
    "### Run the sampler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95450867",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_topics, doc_weights, voc_weights = lda_gibbs_sampler(docs[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf6a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(n_topics):\n",
    "    top_ind = np.argpartition(voc_weights[k, :], -10)[-10:]\n",
    "    print(\"Topic: {0}\".format(k))\n",
    "    print(\", \".join([vectorizer.get_feature_names_out()[ind] for ind in top_ind]))\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
