{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfa8f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow_probability.substrates import numpy as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a750369a",
   "metadata": {},
   "source": [
    "# Mixture Modelling (a.k.a. modelling heterogeneity ie clustering)\n",
    "$\\newcommand{\\iid}{\\stackrel{\\tiny\\mbox{iid}}{\\sim}}$\n",
    "In mixture models, we account for heterogeneity in the data by assuming that there is not \"one single\" data generating process, but actually H of them.\n",
    "\n",
    "Therefore, we expect to find H different subpopulations (== clusters) in our data, and that each subpopulation is homogeneous: it is suitably modeled by a density, typically from a parametric family.\n",
    "\n",
    "In its most general form, let $f_1(\\cdot), \\ldots, f_H(\\cdot)$ be $H$ probability density functions over a space $\\mathbb{Y}, \\mathcal{Y}$, $\\mathbf{w} = (w_1, \\ldots, w_H)$ a vector in the $H$-simplex; a mixture model assumes the following likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "    y_1, \\ldots, y_n \\mid \\mathbf{w}, \\{f_h\\}_{h=1}^H \\iid f^*(y) := \\sum_{h=1}^H w_h f_h(y)\n",
    "    \\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "We refer to the $f_h$'s as the _components_ of the mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff5c81",
   "metadata": {},
   "source": [
    "$\\newcommand{\\ind}{\\stackrel{\\tiny\\mbox{ind}}{\\sim}}$\n",
    "The connection to clustering is made more explicit by introducing \"cluster assignment\" variables $c_i$, $i=1, \\ldots, n$ such that\n",
    "$$\n",
    "    P(c_i = h \\mid \\mathbf{w}) = w_h\n",
    "$$\n",
    "\n",
    "Then, the mixture likelihood can be restated as the following hierarchical model\n",
    "\n",
    "\\begin{equation}\n",
    "    \\begin{aligned}\n",
    "        y_i \\mid c_i = h, \\{f_j\\}_{j=1}^H & \\ind f_h \\\\\n",
    "        P(c_i = h \\mid \\mathbf{w}) = w_h\n",
    "    \\end{aligned}\n",
    "    \\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "Usually, $\\{f_j\\}_{j=1}^H$ belong to the same parametric family (e.g., the normal distribution), and differ only for the specific values of the parameters (e.g., every component has a different mean/variance). But this might not always be the case.\n",
    "\n",
    "**REMARK: Properly justify all your answers!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c57e89",
   "metadata": {},
   "source": [
    "# Exercise 1: Gaussian Mixture Model\n",
    "$\\newcommand{\\iid}{\\stackrel{\\tiny\\mbox{iid}}{\\sim}}$\n",
    "Consider the dataset in the file `gestational_age_data.csv`, that contains the gestational age of more than two thousand children (in days).\n",
    "\n",
    "We can model the data using a mixture of three gaussian distributions.\n",
    "\n",
    "\\begin{equation*}\n",
    "    y_1, \\ldots, y_n \\mid \\mathbf{w}, \\mathbf{\\mu}, \\mathbf{\\sigma}^2 \\iid \\sum_{h=1}^3 w_h \\mathcal{N}(\\mu_h, \\sigma^2_h)\n",
    "\\end{equation*}\n",
    "\n",
    "1.1) Describe a simple prior for $\\mathbf{w}$.\n",
    "  \n",
    "1.2) Using the likelihood in (2), derive the full conditional for $\\mathbf{w}$.\n",
    "\n",
    "1.3) Assume $(\\mu_h, \\sigma_h)$'s to be i.i.d. and, for every $h$, assume a $\\mathcal{N}(\\mu_0, \\sigma^2_0)$ prior for $\\mu_h$ and a Uniform prior over $(l,u)$ for $\\sigma_h$, standard deviation of each mixture component. Specify the values of the prior hyperparameters $(\\mu_0, \\sigma^2_0, l, u)$ considering the empirical mean and standard deviation of the data. Is this prior conjugate?\n",
    "\n",
    "1.4) Derive a sampling strategy to sample from the full conditionals of $(\\mu_h, \\sigma_h)$, for a given $h$.  \n",
    "*(Hint: you may need a step of Metropolis-Hastings: use a Truncated Normal as the proposal distribution. Use the given `step_size` variable as the scale of the proposal)*\n",
    "\n",
    "1.5) Describe a hybrid Gibbs sampler algorithm that alternates between updating the $c_i$'s, the weights $\\mathbf w$ and the \"unique values\" $(\\mu_1, \\sigma_1^2), \\dots, (\\mu_H, \\sigma_H^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ba5802-0413-4793-8077-a91c9765b4a8",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "1.1)\n",
    "\n",
    "1.2)\n",
    "\n",
    "1.3)\n",
    "\n",
    "1.4)\n",
    "\n",
    "1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c7ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"gestational_age_data.csv\")\n",
    "np.random.shuffle(data)\n",
    "data = data[:1000]\n",
    "sns.histplot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec428c",
   "metadata": {},
   "source": [
    "### Prior Elicitation for $\\mu_h, \\sigma_h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aa72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at empirical mean and standard deviation\n",
    "muhat = ??\n",
    "sdhat = ??\n",
    "print(\"Empirical mean: {0}; Empirical sd: {1}\".format(muhat, sdhat))\n",
    "\n",
    "# TODO: Prior elicitation for mu0, sigma2_0, l, u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39163d6e-5658-4d33-8b28-4d033d3381cd",
   "metadata": {},
   "source": [
    "### Sampler implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b609cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_gibbs(data, cluster_allocs, uniq_vals, weights, step_size):\n",
    "    \"\"\"\n",
    "    Runs one iteration of the gibbs sampler\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: a numpy array of floats, contains the observations\n",
    "    cluster_allocs: a numpy array of integers, contains the c_i's\n",
    "        (same length of data)\n",
    "    uniq_vals: a numpy array of size [K, 2], contains (mean, variance)\n",
    "        for every component\n",
    "    weights: a numpy array of size [K], contains the weights of the\n",
    "        components\n",
    "    step_size: a double, contains the step size of the Random Walk Metropolis Hastings\n",
    "    \"\"\"\n",
    "\n",
    "    # Set n_clus\n",
    "    n_clus = len(weights)\n",
    "    # Update Unique Values\n",
    "    for h in range(n_clus):\n",
    "        clusdata = data[cluster_allocs == h]\n",
    "        if len(clusdata) == 0:\n",
    "            uniq_vals[h, :] = sample_uniq_vals_prior()\n",
    "        else:\n",
    "            uniq_vals[h, :] = sample_uniq_vals_fullcond(clusdata, uniq_vals[h, :], step_size)\n",
    "    # Update cluster allocs\n",
    "    cluster_allocs = update_cluster_allocs(data, weights, uniq_vals)\n",
    "    # Update weights\n",
    "    weights = update_weights(cluster_allocs, n_clus)\n",
    "    \n",
    "    return cluster_allocs, uniq_vals, weights\n",
    "\n",
    "\n",
    "\n",
    "def run_mcmc(data, niter=7500, nburn=2500, thin=5):\n",
    "\n",
    "    # Initialize chain\n",
    "    cluster_allocs = tfd.Categorical(probs=np.ones(3) / 3).sample(len(data))\n",
    "    weights = np.ones(3) / 3\n",
    "    step_size = 1e-1\n",
    "    # TODO: Complete by sampling from an appropriate distribution\n",
    "    uniq_vals = np.dstack([\n",
    "        tfd.??.sample(3),\n",
    "        (tfd.??.sample(3))**2])[0, :, :]\n",
    "    \n",
    "    # Prepare buffers\n",
    "    allocs_out = []\n",
    "    uniq_vals_out = []\n",
    "    weights_out = []\n",
    "    \n",
    "    # Sampler for loop\n",
    "    for i in range(niter):\n",
    "        # Sample\n",
    "        cluster_allocs, uniq_vals, weights = run_one_gibbs(\n",
    "            data, cluster_allocs, uniq_vals, weights, step_size)\n",
    "        # Save state\n",
    "        if i > nburn and i % thin == 0:\n",
    "            allocs_out.append(cluster_allocs)\n",
    "            uniq_vals_out.append(uniq_vals)\n",
    "            weights_out.append(weights)\n",
    "        # Print progress\n",
    "        if i % 10 == 9:\n",
    "            print(\"\\rIter {0} / {1}\".format(i+1, niter), flush=True, end=\" \")\n",
    "    \n",
    "    # Return chain\n",
    "    return allocs_out, uniq_vals_out, weights_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ac2952-3e32-4c11-81af-3205e50a8f4c",
   "metadata": {},
   "source": [
    "Each function in the following chunck implements the update of a portion of the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcfd8c5-eccb-45ec-86f8-21dc9cf43b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_probability.substrates import numpy as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "def update_cluster_allocs(data, weights, uniq_vals):\n",
    "    # Hint: the update of each c_i can be done independently of\n",
    "    # the other c_j's (j != i) ---> Use numpy broadcasting /vectorizaiton\n",
    "    # to speed up the code\n",
    "    # Hint: you might need to evaluate the likelihood of each observation\n",
    "    # with parameters in each cluster. The following snippet returns a\n",
    "    # [n, H] matrix with [i, j]-entry equal to log_prob(y_i | \\mu_j, \\sigma^2_j)\n",
    "    \n",
    "    logprobs = tfd.Normal(\n",
    "        uniq_vals[:, 0], np.sqrt(uniq_vals[:, 1])).log_prob(data[:, np.newaxis])\n",
    "    \n",
    "    # TODO: compute probs\n",
    "   \n",
    "    return tfd.Categorical(probs=probs).sample()\n",
    "\n",
    "\n",
    "def update_weights(cluster_allocs, n_clus):\n",
    "    # Hint: you might need to count how many observations are in each cluster\n",
    "    # you can use the following code\n",
    "    # n_by_clus = np.sum(cluster_allocs == np.arange(3)[:, np.newaxis], axis=1)\n",
    "    \n",
    "    # TODO: compute post_params\n",
    "    post_params = None\n",
    "    return tfd.DISTRIBUTION(post_params.astype(float)).sample()\n",
    "\n",
    "\n",
    "def sample_uniq_vals_prior():\n",
    "    mu = tfd.??.sample()\n",
    "    var = (tfd.??.sample()) ** 2\n",
    "    return np.array([mu, var])\n",
    "\n",
    "\n",
    "def sample_uniq_vals_fullcond(clusdata, curr_unique_vals, step_size):\n",
    "    # Get current values\n",
    "    curr_mu = curr_unique_vals[0]\n",
    "    curr_sigma = curr_unique_vals[1]\n",
    "\n",
    "    # TODO: Sample mu\n",
    "    mu = tfd.??.sample()\n",
    "\n",
    "    # TODO: Sample sigma\n",
    "    \n",
    "    def target_lpdf(state, clusdata, loc):\n",
    "        target = ??\n",
    "        return target\n",
    "    \n",
    "    def prop_lpdf(state, loc):\n",
    "        return tfd.??.log_prob(state)\n",
    "    \n",
    "    # TODO: Define curr_state and prop_state\n",
    "    curr_state = np.sqrt(curr_sigma)\n",
    "    prop_state = ??\n",
    "    \n",
    "    # TODO: Compute acceptance rate\n",
    "    log_arate = ??\n",
    "    \n",
    "    # Test if prop_state is accepted\n",
    "    if np.log(tfd.Uniform(0,1).sample()) < log_arate:\n",
    "        var = prop_state ** 2\n",
    "    else:\n",
    "        var = curr_state ** 2\n",
    "\n",
    "    # Return [mu, var]\n",
    "    return np.array([mu, var])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3ed096-407c-470c-aefd-fec3e4d876d3",
   "metadata": {},
   "source": [
    "### Run the Sampler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e547d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "allocs_out, uniq_vals_out, weights_out = run_mcmc(data, niter=1500, nburn=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052bf8bf-b0ac-4870-adbd-6bac4ff4b859",
   "metadata": {},
   "source": [
    "Let's see an example of clustering by plotting the last values of `allocs_out` against the data histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a97d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data, density=True, alpha=0.1)\n",
    "for h in range(3):\n",
    "    currd = data[allocs_out[-1] == h]\n",
    "    plt.scatter(currd, np.zeros_like(currd) + 0.001 * (h+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e49229-58e9-473a-a8ed-da39f6fd9de7",
   "metadata": {},
   "source": [
    "## Hint\n",
    "If you want to do Exs 1 & 2 in parallel, you can simulate a fake Markov Chain that mimics the output of Ex1.\n",
    "\n",
    "For $j=1, \\ldots, M$ sample:\n",
    "\n",
    "\\begin{align}\n",
    "    &\\mu_1^{(j)} \\sim \\mathcal{N}(-5, 0.5), \\quad \\mu_2^{(j)} \\sim \\mathcal{N}(5, 0.5); \\\\[2pt]\n",
    "    &\\sigma^{2, (j)}_1 \\sim \\mathcal{IG}(3,3), \\quad \\sigma^{2, (j)}_2 \\sim \\mathcal{IG}(3,3); \\\\[2pt]\n",
    "    &c_i^{(j)} \\sim \\text{Categorical(0.9, 0.1)} \\text{, for } i = 1, \\dots, 50; \\\\[2pt]\n",
    "    &c_i^{(j)} \\sim \\text{Categorical(0.1, 0.9)} \\text{, for } i = 51, \\ldots, 100; \\\\[2pt]\n",
    "    &\\mathbf{w}^{(j)} \\sim \\text{Dirichlet}(5, 5).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8082580",
   "metadata": {},
   "source": [
    "# Exercise 2: Posterior inference in Mixture Models\n",
    "\n",
    "Mixture models are subject to the so called \"label switching\", which causes non-identifiability for all the parameters.\n",
    "\n",
    "Label switching means that permuting the \"labels\" {1, 2, ..., K} of the cluster yields the same exact likelihood.\n",
    "Hence, it does not make sense to talk about \"first\", \"second\" and \"third\" cluster.\n",
    "Moreover, also interpretation of the cluster parameters is tricky: we cannot say that\n",
    "\n",
    "$$\n",
    "    \\frac{1}{M} \\sum_{j=1}^M \\mu^{(j)}_1\n",
    "$$\n",
    "\n",
    "is an estimate of parameter $\\mu_1$. Also, it does not make sense to take averages of the cluster allocation labels.\n",
    "\n",
    "There are several ways to solve this non-identifiability issue, we will focus here on a decision-theoretic approach.\n",
    "\n",
    "\n",
    "2.1) Implement a function that finds the clustering, among the ones visited in the MCMC chain, that minimizes Binder's loss function:\n",
    "\n",
    "$$\n",
    "    \\sum_{i=1}^n \\sum_{j=1}^n (\\mathbb I [c_i = c_j] - p_{ij})^2\n",
    "$$\n",
    "where $p_{ij}$ is the posterior probability that observations $i$ and $j$ belong to the same cluster.\n",
    "\n",
    "Note that $p_{ij}$ can be easily estimated from the MCMC chains!\n",
    "\n",
    "2.2) Given the estimated clustering $\\mathbf{c}^*$, estimate the parameters in each cluster. Let $c^*_h$ denote the index-set of the $h$--th cluster (e.g., $c^*_1 = \\{1, 3, 10\\}$, $c^*_2 = \\{0, 2, 4, 5, 6\\}$ ...)\n",
    "Then\n",
    "$$\n",
    "    \\hat \\mu_h = \\frac{1}{M} \\sum_{j=1}^M \\frac{1}{\\# c^*_h} \\sum_{i \\in c^*_h} \\mu^{(j)}_{c_{i}^{(j)}}\n",
    "$$\n",
    "where $\\mu^{(j)}_\\ell$ is the mean of the $\\ell$--th cluster at the $j$--th MCMC iteration.\n",
    "\n",
    "Similarly for the variance $\\hat \\sigma_h^2$.\n",
    "\n",
    "2.3) Comment on the posterior inference obtained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173a9811-7be7-4fcf-a310-d50ed78021b1",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98fbd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_psm(clus_alloc_chain):\n",
    "    \"\"\"\n",
    "    Returns the posterior similarity matrix, i.e.\n",
    "        out[i, j] = P(c_i == c_j)\n",
    "    for each pair of observations\n",
    "    \"\"\"\n",
    "    c_chain = np.vstack(clus_alloc_chain)\n",
    "    out = np.zeros((c_chain.shape[1], c_chain.shape[1]))\n",
    "    \n",
    "    # TODO: fill out!\n",
    "    \n",
    "    return out + out.T + np.eye(out.shape[0])\n",
    "\n",
    "\n",
    "def minbinder_sample(clus_alloc_chain, psm):\n",
    "    losses = np.zeros(len(clus_alloc_chain))\n",
    "    c_chain = np.vstack(clus_alloc_chain)\n",
    "    \n",
    "    # You can either cycle through the iterations, or \n",
    "    # cycle through the entries in the PSM [i, j]\n",
    "    # and vectorize the same operation for each iteration!\n",
    "    \n",
    "    # TODO: compute the losses!\n",
    "    \n",
    "    best_iter = np.argmin(losses)\n",
    "    return clus_alloc_chain[best_iter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec7b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "psm = get_psm(allocs_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0440dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clus = minbinder_sample(allocs_out, psm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c444c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data, density=True, alpha=0.3)\n",
    "for h in range(3):\n",
    "    currd = data[best_clus == h]\n",
    "    plt.scatter(currd, np.zeros_like(currd) + 0.001 * (h+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015eb95-ab4d-415d-869f-d9a84eb62008",
   "metadata": {},
   "source": [
    "2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a58ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_vals_given_clus(unique_vals_chain, clus_alloc_chain, best_clus):\n",
    "    c_allocs = np.stack(clus_alloc_chain)\n",
    "    uniq_vals = np.stack(unique_vals_chain)\n",
    "    means = uniq_vals[:, :, 0]\n",
    "    variances = uniq_vals[:, :, 1]\n",
    "    out = []\n",
    "    \n",
    "    for h in range(len(np.unique(best_clus))):\n",
    "        data_idx = np.where(best_clus == h)[0]\n",
    "        uniq_vals_idx = c_allocs[:, data_idx] # -> Matrix [n_iter x n_data_in_clus]\n",
    "        means_by_iter = np.empty((c_allocs.shape[0], len(data_idx)))\n",
    "        vars_by_iter = np.empty_like(means_by_iter)\n",
    "        for i in range(c_allocs.shape[0]):\n",
    "            means_by_iter[i, :] = ??\n",
    "            vars_by_iter[i, :] = ??\n",
    "\n",
    "        avg_mean_by_iter = ??\n",
    "        avg_var_by_iter = ??\n",
    "        \n",
    "        muhat = np.mean(avg_mean_by_iter)\n",
    "        sigsqhat = np.mean(avg_var_by_iter)\n",
    "        out.append(np.array([muhat, sigsqhat]))\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "clus_vals = unique_vals_given_clus(uniq_vals_out, allocs_out, best_clus)\n",
    "clus_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e136ca4-8539-4fdb-b54c-fd089ee94d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.sum(best_clus == np.arange(3)[:, np.newaxis], axis=1) / len(data)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data, density=True, alpha=0.1, bins=30)\n",
    "xx = np.linspace(np.min(data), np.max(data), 1000)\n",
    "out_dens = np.zeros_like(xx)\n",
    "for i in range(3):\n",
    "    comp_dens = tfd.Normal(clus_vals[i][0], np.sqrt(clus_vals[i][1])).prob(xx)\n",
    "    out_dens += weights[i] * comp_dens\n",
    "    plt.fill_between(xx, np.zeros_like(xx), weights[i] * comp_dens, alpha=0.6)\n",
    "\n",
    "plt.plot(xx, out_dens, c=\"black\", linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca74a0d0-2bd7-419d-8bf3-a549b7b63f57",
   "metadata": {},
   "source": [
    "2.3) **Comments on the inference obtained**  \n",
    "The three clusters differ in...  \n",
    "Their weights are ...  \n",
    "Hence we can conclude that the population ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9f91c",
   "metadata": {},
   "source": [
    "# Exercise 3: Latent Dirichlet Allocation\n",
    "\n",
    "We now move to one of the most fundamental papers in Machine Learning / Text Mining: Latent Dirichlet Allocations by [Blei et al (2003)](https://dl.acm.org/doi/10.5555/944919.944937).\n",
    "\n",
    "The goal is topic modelling given a corpus (= collection of documents)\n",
    "\n",
    "Denote with $\\mathbf y_i = \\{y_{i1}, \\ldots, y_{i N_i}\\}$ the $i$--th document. Let $|V|$ be the size of the vocabulary (number of unique words) and $K$ the number of topics.\n",
    "\n",
    "For each topic $k$, we assume a different distribution over the vocabulary $\\beta_k = (\\beta_{k,1}, \\ldots \\beta_{k, |V|})$.\n",
    "\n",
    "Moreover, in each document more than one topic can be discussed. Denote with $w_i = (w_{i, 1}, \\ldots, w_{i, K})$ the weight of each topic in the $i$--th document.\n",
    "\n",
    "Then LDA assumes\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    P(y_{i, j} = v \\mid \\{\\beta_k\\}_{k=1}^J, w_i) &= \\sum_{k=1}^K w_{i, k} \\beta_{k, v}, \\quad v=1, \\ldots, |V| \\\\\n",
    "    \\beta_k &\\sim \\text{Dirichlet}(\\alpha_1, \\ldots, \\alpha_{|V|}) \\\\\n",
    "    w_i & \\sim \\text{Dirichlet}(0.5, \\ldots, 0.5)\n",
    "\\end{aligned}\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "3.1) Rewrite the likelihood introducing suitable auxiliary variables $t_{ij}$ so that $w_i$ does appear on the right hand side of the first equation in (3)\n",
    "\n",
    "3.2) Specify a prior for the $\\beta_j$'s such that, a priori, the expected value of $\\beta_{j, v}$ is proportional to the number of times word $v$ appears in the corpus. What about the variance?\n",
    "\n",
    "3.3) Implement a Gibbs sampler to perform posterior inference. Instead of identifying the mixture parameters, consider only the last iteration of the MCMC. What can we say about the topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066864e8-664f-466c-85c5-91e0d9b20c8a",
   "metadata": {},
   "source": [
    "## Answers (trace)\n",
    "\n",
    "3.1) We introduce $t_{ij} \\sim \\text{TODO}$ (one for each $y_{ij}$) so that\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    P(y_{ij} = v \\mid \\{\\beta_k\\}, t_{ij} = h) &= \\beta_{h, v} \\\\\n",
    "    P(t_{ij} = h \\mid \\pi_i) &= \\text{TODO}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "3.2) We fix the expected value as ...  \n",
    "To control the variance, call $d_v =  \\sum_{i=1}^D \\sum_{j=1}^{N_i} \\mathbb{I}[y_{ij} = v]$, so that $\\alpha_v = \\kappa d_v$.\n",
    "\n",
    "3.3) The joint model is\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    \\mathcal{L}(\\{y_{ij}\\}, \\{\\beta_k\\}, \\{w_i\\}, \\{t_{ij}\\}) &= \\prod_{i=1}^D \\prod_{j=1}^{N_i} w_{i, t_{ij}} \\prod_{v=1}^{|V|} \\beta_{t_{ij}, v}^{\\mathbb{I}[y_{ij} = v]} \\\\\n",
    "    & \\times \\prod_{k=1}^K \\frac{1}{B(\\mathbf \\alpha)} \\prod_{v=1}^{|V|} \\beta_{k, v}^{\\alpha_v - 1} \\mathbb{I}[\\beta_k \\in S^{|V|}] \\\\\n",
    "    & \\times \\prod_{i=1}^D \\frac{1}{B(\\mathbf{0.5})} \\prod_{k=1}^{K} w_{i, k}^{0.5 - 1} \\mathbb{I}[w_i \\in S^{K}]\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Then it is easy to see that\n",
    "\n",
    "1) $\\mathcal{L}(w_i \\mid \\cdots) \\propto \\prod_{k=1}^{K} \\text{TODO}$\n",
    "\n",
    "   *(Hint: rewrite $\\prod_{j=1}^{N_i} w_{i, t_{ij}} = \\prod_{k=1}^K w_{i, k}^{n_{i, k}}$, where $n_{i, k} = ??$)*  \n",
    "   Finally, conclude that $w_i \\mid \\cdots \\sim \\text{TODO}$\n",
    "\n",
    "\n",
    "2) $ \\mathcal{L}(\\beta_k \\mid \\cdots) \\propto \\text{TODO} $\n",
    "   \n",
    "   Hence $\\beta_k \\mid \\cdots \\sim \\text{TODO}$\n",
    "\n",
    "   \n",
    "3) If $y_{ij} = v^*$, $P(t_{ij} = k \\mid \\cdots) \\propto w_{ik} \\beta_{k, v^*}, \\quad k=1, \\ldots, K$\n",
    "   \n",
    "   Hence, $t_{ij}$ is categorically-distributed with probabilities proportional to $w_{ik} \\beta_{k, v^*}$. We can sample $t_{ij}$ from a Categorical distribution after having computed the unnormalized probabilities for each $k$ and having normalized them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f6f452-0477-4085-a747-e83b40e6a26f",
   "metadata": {},
   "source": [
    "## Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d2f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install scikit-learn if not available\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups()[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903b1826",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b52b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some preprocessing.\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"Removes digits and weird punctuation\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\__', '', text)\n",
    "    text = re.sub(r'\\___', '', text)\n",
    "    return text\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    preprocessor=preprocess_text,\n",
    "    stop_words=\"english\",\n",
    "    strip_accents=\"unicode\",\n",
    "    max_df=0.95,\n",
    "    min_df=0.01,\n",
    "    max_features=500)\n",
    "\n",
    "vectorizer.fit(data)\n",
    "print(vectorizer.get_feature_names_out()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a620e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = vectorizer.build_analyzer()\n",
    "docs = []\n",
    "for doc in data:\n",
    "    tokens = analyzer(doc)\n",
    "    idxs = np.array(list(filter(\n",
    "        lambda x: x, \n",
    "        [vectorizer.vocabulary_.get(tok, None) for tok in tokens])))\n",
    "    docs.append(idxs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc85eb5-18ef-4c2d-a0a1-0fc4239cbdfc",
   "metadata": {},
   "source": [
    "**`docs` is your final dataset, work only with it!** `docs` is a list where every entry represents a document. A document is represented as a list of indexes (each work represents a word)\n",
    "\n",
    "### Sampler implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f6e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampler quantities\n",
    "n_topics = 20\n",
    "voc_len = len(vectorizer.get_feature_names_out())\n",
    "alpha = np.ones(voc_len) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a5a5f7-2080-4dd7-980d-61632d596504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_gibbs_sampler(docs):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs: a list of np.arrays of integers, each represnts the \n",
    "        words in a document\n",
    "        \n",
    "    word_topics are the t_{ij}'s\n",
    "    voc_weights are the \\beta_k's\n",
    "    doc_weights are the w_i's\n",
    "    \"\"\"\n",
    "    \n",
    "    word_topics = [\n",
    "        tfd.Categorical(probs=np.ones(n_topics)).sample(len(x)) for x in docs]\n",
    "    voc_weights = tfd.Dirichlet(alpha).sample(n_topics)\n",
    "    doc_weights = tfd.Dirichlet(np.ones(n_topics) * 0.5).sample(len(docs))\n",
    "    \n",
    "    for i in range(100):\n",
    "        print(\"\\rIter {0} / {1}\".format(i+1, 100), flush=True, end=\" \")\n",
    "        word_topics, doc_weights, voc_weights = run_one_lda(\n",
    "            docs, word_topics, doc_weights, voc_weights)\n",
    "    return word_topics, doc_weights, voc_weights\n",
    "\n",
    "\n",
    "def run_one_lda(docs, word_topics, doc_weights, voc_weights):    \n",
    "    word_topics = update_word_topics(doc_weights, voc_weights, docs)\n",
    "    doc_weights = update_doc_weights(word_topics)\n",
    "    voc_weights = update_voc_weights(docs, word_topics)\n",
    "    return word_topics, doc_weights, voc_weights\n",
    "\n",
    "\n",
    "def update_voc_weights(docs, word_topics):\n",
    "    \"\"\"Updates \\beta_k for every k\"\"\"\n",
    "    out = np.zeros((n_topics, voc_len))\n",
    "    for k in range(n_topics):\n",
    "        # Get all the words whose topic is k (i.e. t_ij = k)\n",
    "        # Kept words is a np.array of integers with the indexes/words\n",
    "        # of all y_{ij} for which t_{ij} = k\n",
    "        mask = [np.where(x == k) for x in word_topics]\n",
    "        kept_words = np.concatenate([x[y] for x, y in zip(docs, mask)])\n",
    "        \n",
    "        # Returns the unique values in kept_words and their counts\n",
    "        # For instance if kept_words = [1, 1, 1, 0, 5, 4]\n",
    "        # --> idxs = [0, 1, 4, 5]\n",
    "        # --> counts = [1, 3, 1, 1]\n",
    "        idxs, counts = np.unique(kept_words, return_counts=True)\n",
    "        \n",
    "        dir_param = ???\n",
    "        out[k] = tfd.DISTRIBUTION(dir_param).sample()\n",
    "    return out\n",
    "\n",
    "\n",
    "def update_doc_weights(word_topics):\n",
    "    \"\"\"Updates w_i for every i\"\"\"\n",
    "    out = np.zeros((len(docs), n_topics))\n",
    "    for doc_ind, word_topic_in_doc in enumerate(word_topics):\n",
    "        # word_topic_in_doc is the doc_ind-th row of word_topics\n",
    "        \n",
    "        # cnt returns how many times each topic appears in each document\n",
    "        # i.e. cnt[k] is the number of words ind the doc_ind-th document\n",
    "        # that have topic k\n",
    "        cnt = np.sum(\n",
    "            word_topic_in_doc == np.arange(n_topics)[:, np.newaxis], axis=1)\n",
    "        \n",
    "        dir_param = ???\n",
    "        out[doc_ind] = tfd.DISTRIBUTION(dir_param).sample()\n",
    "    return out\n",
    "\n",
    "\n",
    "def update_word_topics(doc_weights, voc_weights, docs):\n",
    "    out = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        # computes an [num_words x K] matrix with entries\n",
    "        # probas[l, m] beta_{m, v_l} * w_{i, m}\n",
    "        # where v_l is the word in position 'l'\n",
    "        # --> for each word (by row) represents the unnormalized probability\n",
    "        #     of each topic\n",
    "        probas = doc_weights[i][np.newaxis, :] * voc_weights[:, doc].T\n",
    "        \n",
    "        # normalize it approriately\n",
    "        probas = ???\n",
    "        out.append(tfd.Categorical(probs=probas).sample())\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590319b-124b-4bb8-a0a9-ff7db51d6db0",
   "metadata": {},
   "source": [
    "### Run the sampler!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95450867",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_topics, doc_weights, voc_weights = lda_gibbs_sampler(docs[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bf6a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(n_topics):\n",
    "    top_ind = np.argpartition(voc_weights[k, :], -10)[-10:]\n",
    "    print(\"Topic: {0}\".format(k))\n",
    "    print(\", \".join([vectorizer.get_feature_names_out()[ind] for ind in top_ind]))\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
